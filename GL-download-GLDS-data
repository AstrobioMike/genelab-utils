#!/usr/bin/env python

"""
This is a helper program for downloading GLDS data files.
"""

import os
import sys
import argparse
import textwrap
import pandas as pd
from time import sleep
import subprocess
from json import loads
from urllib.request import urlopen
import tempfile

parser = argparse.ArgumentParser(description = "This is a program for downloading GLDS data files. For version info, run \
                                                `GL-version`.",
                                 epilog = "\033[0;33m\
                                            NOTICE: Some confusion may arise due to recent changes. It is possible that \
                                            a GLDS ID and an OSD ID may not match up, e.g., 'OSD-561' (https://osdr.nasa.gov/bio/repo/data/studies/OSD-561) \
                                            holds 'GLDS-556' (which we can see at the very top, just under the image next to the title). Moving forward, \
                                            IT IS RECOMMENDED to search for the OSD ID (which you can search for based on a given GLDS ID here: https://osdr.nasa.gov/bio/repo/search) – \
                                            as that will find all the associated GLDS files no matter what their GLDS ID's are. E.g., 'GL-download-GLDS-data -g OSD-561 --print-only'. \
                                            Contact Mike Lee at Mike.Lee@nasa.gov if having trouble. \
                                            \
                                          \033[0m")

required = parser.add_argument_group('required arguments')

required.add_argument("-g", "--OSD-or-GLDS-ID", help='OSD ID (e.g., "ODS-276") or GLDS ID (e.g. "GLDS-276"); be sure to read the NOTICE at the bottom of the help menu', 
                      action="store", required = True, type = str)

required.add_argument("-p", "--pattern", help = "If we do not want to download all files (which we often won't), we can specify \
                      a pattern here to subset the total files. For example, if we know we want to download just the fastq.gz \
                      files, we can say '-p fastq.gz'. We can also provide multiple patterns as a comma-separated list. For example, \
                      If we want to download the fastq.gz files that also have 'NxtaFlex', 'metagenomics', and 'raw' in their filenames, we can provide \
                      '-p fastq.gz,NxtaFlex,metagenomics,raw'. Looking through the *-file-info.tsv table produced by this program when run \
                      with the '--just-get-file-info-table' flag can help figure this out if needed. (Note that this is case-sensitive.)", action = "store", type = str)

parser.add_argument("-j", "--jobs", help = "Number of downloads to run in parallel (default: 10)", default = 10, action = "store", type = int)

parser.add_argument("-P", "--print-only", help = "Just print out the files that would be downloaded, rather than downloading them \
                                                  (useful to check we are getting what we want first)", action = "store_true")

parser.add_argument("-f", "--force", help = "Don't ask for confirmation to begin download (helpful if wanting to \
                                                  run non-interactively)", action = "store_true")

parser.add_argument("--just-get-file-info-table", help = "Just download a table of all available files and their URLs \
                                                          (doesn't incorporate pattern searching)", action = "store_true") 

parser.add_argument("-m", "--mode", choices = {"exclusive", "additive"}, help = "If providing multiple patterns to the `-p` argument, \
                    this option determines how to handle them. For example, if we provide `-p raw,fastq`, in the default \
                    mode of 'exclusive', we will only grab files that have *both* 'raw' and 'fastq' in their filenames. If we set \
                    `-m additive`, we would get all files that have *either* 'raw' or ' fastq' in their filenames. (default: exclusive)", 
                    default = "exclusive", action = "store", type = str)


if len(sys.argv) == 1:
    parser.print_help(sys.stderr)
    sys.exit(0)

args = parser.parse_args()

# if input was provided as GLDS, changing to OSD due to update to data repository
OSD_acc = args.OSD_or_GLDS_ID.replace("GLDS", "OSD")

GLDS_file_json_url = "https://genelab-data.ndc.nasa.gov/genelab/data/glds/files/" + str(OSD_acc).removeprefix("OSD-")

base_file_url_prefix = "https://genelab-data.ndc.nasa.gov"

GLDS_file_info_output_tab = OSD_acc + "-file-info.tsv"

download_commands_file = str(OSD_acc) + "-wanted-file-download-commands.sh"
not_downloaded_files_file = str(OSD_acc) + "-files-targeted-but-not-downloaded.txt"

################################################################################

def main():

    if args.OSD_or_GLDS_ID.startswith("GLDS-"):

        confirm_continue_with_GLDS_search()

    # making sure input fits expected format
    if not args.OSD_or_GLDS_ID.startswith("GLDS-"):
        
        if not args.OSD_or_GLDS_ID.startswith("OSD-"):

            report_message("The input --GLDS-ID argument needs to be in the format of 'GLDS-' or 'OSD-' followed by the appropriate accession number.")
            print("")
            exit(1)


    # getting GLDS file-info into table
    report_message("Attempting to retrieve '" + args.OSD_or_GLDS_ID + "' file data from:")
    print("    " + GLDS_file_json_url + "\n")
    
    gen_files_table(GLDS_file_json_url)

    # exiting if user just wants file-info table
    if args.just_get_file_info_table:
        exit()

    # reading in file-info table
    in_tab = pd.read_csv(GLDS_file_info_output_tab, sep = "\t")
    
    # number of total files
    num_total_files = len(in_tab.index)
    
    # list of all files
    all_files = in_tab.filename.tolist()

    report_message(f"The downloaded table holds a total of {num_total_files} files.")
    print("")
    sleep(1)

    # getting targets
    target_files, num_files_to_download = get_targets(all_files, num_total_files)

    if args.print_only:

        print_target_files(target_files, num_files_to_download)

    if not args.force:

        confirm_download(num_files_to_download)

    # building and performing download
    download(target_files, in_tab, num_files_to_download)

    # checking expected files were downloaded
    check_expected_files_were_downloaded(target_files, num_files_to_download)

################################################################################

# setting some colors
tty_colors = {
    'green' : '\033[0;32m%s\033[0m',
    'yellow' : '\033[0;33m%s\033[0m',
    'red' : '\033[0;31m%s\033[0m'
}


### functions ###
def color_text(text, color='green'):
    if sys.stdout.isatty():
        return tty_colors[color] % text
    else:
        return text


def wprint(text):
    """ print wrapper """

    print(textwrap.fill(text, width=80, initial_indent="  ",
          subsequent_indent="  ", break_on_hyphens=False))


def report_failure(message, color = "yellow"):
    print("")
    wprint(color_text(message, color))
    print("\nData file info download failed.\n")
    sys.exit(1)


def report_message(message, color = "yellow"):
    print("")
    wprint(color_text(message, color))


def gen_files_table(files_info_url):

    """
    This function access the files info json from https://genelab-data.ndc.nasa.gov/genelab/data/glds/files/<OSD_ID>
    and creates a table with filenames and links
    """

    # reading fileslisting json
    with urlopen(files_info_url) as json:

        files_json = loads(json.read().decode())

    try:
        files_dict = files_json["studies"][OSD_acc]["study_files"]

    except KeyError:

        report_message(f"The input --OSD-or-GLDS-ID argument, '{args.OSD_or_GLDS_ID}', was not found :(", "yellow")
        report_message("See NOTICE in help menu in case it's relevant to this case ('GL-download-GLDS-data -h').")
        print("")
        exit(1)

    # writing out table with filenames and their urls
    with open(GLDS_file_info_output_tab, "w") as out_file:

        # starting header
        out_file.write("filename\turl\n")

        for entry in files_dict:

            filename = entry["file_name"]
            
            file_url = os.path.join(base_file_url_prefix, entry["remote_url"].lstrip("/"))

            out_file.write(f"{filename}\t{file_url}\n")

    # writing out file-info table
    report_message("A table with available filenames and URLs has been written to:")
    print("    " + GLDS_file_info_output_tab + "\n")
    sleep(1)


def check_expected_files_were_downloaded(target_files, num_files_to_download):

    """ this function checks the expected were downloaded """

    cwd_files = os.listdir(".")

    missing_files = []

    for file in target_files:

        if file not in cwd_files:

            missing_files.append(file)

    num_missing = len(missing_files)

    if num_missing > 0:

        with open(not_downloaded_files_file, "w") as out_file:
            for file in missing_files:
                out_file.write(f"{file}\n")

        report_message(f"Some file ({num_missing}) weren't successfully downloaded for some reason. They have been written to:")
        print(f"    {not_downloaded_files_file}\n")

    else:

        report_message(f"All {num_files_to_download} file(s) have been successfully downloaded :)", "green")
        print("")

    report_message(f"The download commands executed were written to:")
    print(f"    {download_commands_file}\n")


def get_targets(all_files, num_total_files):

    if args.pattern:

        # splitting if any commas
        if "," in args.pattern:

            patterns = args.pattern.split(",")

        else:

            patterns = [args.pattern]

        # starting empty list of target files that hold the searched patterns
        target_files = []

        for file in all_files:

            # dealing with if mode is exclusive or additive (see help)
            if args.mode == "exclusive":

                if all(pattern in file for pattern in patterns):

                    target_files.append(file)

            else:

                if any(pattern in file for pattern in patterns):

                    target_files.append(file)


        # getting rid of any duplicates
        target_files = set(target_files)
        target_files = list(target_files)

        # number of files after filtering
        num_files_to_download = len(target_files)
        
        if num_files_to_download == 0:

            report_message(f"No files were found matching the specified pattern(s) :(")
            report_message(f"Maybe look into the '{GLDS_file_info_output_tab}' file to see what's there.")
            report_message(f"(Also note that the pattern-matching done here is case-sensitive.)")
            print("")
            exit(0)

        report_message(f"{num_files_to_download} file(s) found matching the provided pattern(s).")
        print("")
        sleep(1)

    else:
        num_files_to_download = num_total_files
        target_files = all_files

    return(target_files, num_files_to_download)


def download(target_files, in_tab, num_files_to_download):

    """ this function prepares and performed the download with curl/parallel in subprocesses """

    # building download script based on curl
    beginning_of_command = "curl -L -s -o"

    # putting commands into list also
    download_commands_list = []

    with open(download_commands_file, "w") as download_script:

        for file in target_files:

            target_url = in_tab.loc[in_tab['filename'] == file, 'url'].iloc[0]

            download_script.write(f"{beginning_of_command} {file} '{target_url}'\n")

            download_commands_list.append(f"{beginning_of_command} {file} '{target_url}'")


    report_message(f"Beginning download of the {num_files_to_download} file(s)...")
    print("")

    ## splitting into chunks the size of jobs, so can update at least somewhat during the download
    split_download_commands_list = [download_commands_list[command:command + args.jobs] for command in range(0, len(download_commands_list), args.jobs)]
    
    ## downloading and reporting for each block
    # counter for files downloaded
    files_processed = 0
    print(f"    Files processed: {files_processed} out of {num_files_to_download} target(s)...", end = '\r')

    for block in split_download_commands_list:

        # writing to tempfile for passing to curl/parallel
        curr_temp_file = tempfile.NamedTemporaryFile()
        
        with open(curr_temp_file.name, mode = "w", encoding = "utf-8") as curr_dl_file:

            curr_dl_file.write("\n".join(block))
            curr_dl_file.write("\n")

        # downloading this block
        dl_command = f"parallel --xapply -j {args.jobs} < {curr_temp_file.name}"
        subprocess.run(dl_command, shell = True)

        # incrementing file counter
        files_processed += len(block)

        # reporting 
        if files_processed != num_files_to_download:

            print(f"    Files processed: {files_processed} out of {num_files_to_download} target(s)...", end = '\r')

        else:
            print(f"    Files processed: {files_processed} out of {num_files_to_download} target(s)...", end = '\r')
            print("\n")
            sleep(1.5)


def print_target_files(target_files, num_files_to_download):

    report_message(f"As requested, here are the {num_files_to_download} files that would be downloaded by this command if run without the '--print-only' flag:")
    print()
    sleep(2.5)

    for filename in target_files:
        print(f"    {filename}")
        sleep(0.01)

    print("")
    exit(0)


def confirm_download(num_files_to_download):

    report_message(f"If you'd just like to see a list of the file(s) that would be downloaded, here is your chance to exit and rerun the command with the '--print-only' flag...")
    sleep(1)
    print("")

    print(f"    Enter 'y' if you'd like to begin downloading {num_files_to_download} file(s),")
    response = input("    enter any other key to exit without downloading: ")

    if response != "y":

        report_message(f"Exiting without downloading :)")
        print("")
        exit(0)

    else:

        print("")


def confirm_continue_with_GLDS_search():

    report_message("NOTICE!")
    report_message("We see that you are searching by a GLDS-ID, and we want to note that some confusion may arise due to recent changes.")
    report_message("It is possible that \
a GLDS ID and an OSD ID may not match up, e.g., 'OSD-561' (https://osdr.nasa.gov/bio/repo/data/studies/OSD-561) \
holds 'GLDS-556' (we can see the GLDS ID at the very top of that page, just under the image next to the title).")
    report_message("Moving forward, IT IS RECOMMENDED to search for the OSD ID (which you can search for based on a given GLDS ID here: https://osdr.nasa.gov/bio/repo/search) \
– as that will find all the associated GLDS files no matter what their GLDS ID's are. E.g., 'GL-download-GLDS-data -g OSD-561 --print-only'.")
    report_message("This is only a notice, and you are free to move forward with searching by GLDS ID, but be sure to check visually against the \
appropriate repository page that you are getting all the desired files, and consider using the OSD ID instead :)")
    report_message("Contact Mike Lee at Mike.Lee@nasa.gov if having trouble.")

    sleep(3)
    print("")

    print(f"    Enter 'y' if you'd like to continue with the GLDS-ID search,")
    response = input("    or enter any other key to exit and re-do the search based on the OSD ID (recommended): ")

    if response != "y":

        report_message(f"Exiting for now :)")
        report_message("Opening this page might get you started on finding the appropriate OSD ID to search with, if the input GLDS-ID can be found: ")
        report_message(f"    https://osdr.nasa.gov/bio/repo/search?q={args.OSD_or_GLDS_ID}")
        print("")
        exit(0)

    else:

        print("")

    exit()


if __name__ == "__main__":
    main()
