#!/usr/bin/env python

"""
This is a program for generating the 2 JIRA tables needed for GeneLab metagenomics processed datasets for Curation.
"""

import os
import sys
import argparse
import textwrap
import pandas as pd
import tarfile
import zipfile
import glob

parser = argparse.ArgumentParser(description="This program generates the 2 JIRA tables needed for GeneLab metagenomics \
                                             processed datasets to be passed onto Curation.\
                                             Hard-coded variables that may need to be changed are at the top \
                                             of the script. It is expected to be run after `GL-validate-metagenomics` and \
                                             `GL-gen-metagenomics-readme` have been run successfully.")

required = parser.add_argument_group('required arguments')

required.add_argument("-g", "--GLDS-ID", help='GLDS ID (e.g. "GLDS-276")', action="store", required = True)
required.add_argument("-i", "--isa-zip", help='Appropriate ISA file for dataset (zipped)', action="store", required = True)

if len(sys.argv)==1:
    parser.print_help(sys.stderr)
    sys.exit(0)

args = parser.parse_args()


### hard-coded stuff we might want to change ###
file_prefix = args.GLDS_ID + "_GMetagenomics_"

raw_reads_dir = "Raw_Sequence_Data/"
fastqc_dir = "FastQC_Outputs/"
filtered_reads_dir = "Filtered_Sequence_Data/"
assembly_based_dir = "Assembly-based_Processing/"
assemblies_dir = "Assembly-based_Processing/assemblies/"
genes_dir = "Assembly-based_Processing/predicted-genes/"
annotations_and_tax_dir = "Assembly-based_Processing/annotations-and-taxonomy/"
mapping_dir = "Assembly-based_Processing/read-mapping/"
combined_output_dir = "Assembly-based_Processing/combined-outputs/"
bins_dir = "Assembly-based_Processing/bins/"
MAGs_dir = "Assembly-based_Processing/MAGs/"
read_based_dir = "Read-based_Processing/"

raw_R1_suffix = "_R1_raw.fastq.gz"
raw_R2_suffix = "_R2_raw.fastq.gz"
filtered_R1_suffix = "_R1_filtered.fastq.gz"
filtered_R2_suffix = "_R2_filtered.fastq.gz"
assembly_suffix = "-assembly.fasta"

processing_tar_file = "processing_info.tar"

################################################################################

def main():

    check_for_file_and_contents(args.isa_zip)

    samples_in_ISA_order = get_samples_from_ISA(args.isa_zip)

    read_counts_df = get_read_counts_from_raw_multiqc(samples_in_ISA_order)

#    gen_and_write_out_QC_metadata_tab(samples_in_ISA_order, read_counts_df)

    gen_and_write_out_filenames_table(samples_in_ISA_order, read_counts_df)

################################################################################


# setting some colors
tty_colors = {
    'green' : '\033[0;32m%s\033[0m',
    'yellow' : '\033[0;33m%s\033[0m',
    'red' : '\033[0;31m%s\033[0m'
}


### functions ###
def color_text(text, color='green'):
    if sys.stdout.isatty():
        return tty_colors[color] % text
    else:
        return text


def wprint(text):
    """ print wrapper """

    print(textwrap.fill(text, width=80, initial_indent="  ",
          subsequent_indent="  ", break_on_hyphens=False))


def report_failure(message, color = "yellow"):
    print("")
    wprint(color_text(message, color))
    print("\nJIRA-table generation failed.\n")
    sys.exit(1)


def check_for_file_and_contents(file_path):
    """ used by check_fastq_files function """

    if not os.path.exists(file_path):
        report_failure("The expected file '" + str(file_path) + "' does not exist.")
    if not os.path.getsize(file_path) > 0:
        report_failure("The file '" + str(file_path) + "' is empty.")


def get_samples_from_ISA(isa_file):
    """ gets the sample names in their order from the ISA zip file """

    zip_file = zipfile.ZipFile(isa_file)
    isa_files = zip_file.namelist()

    # getting wanted filename (those that start with "a_" seem to be what we want)
    wanted_file_list = [item for item in isa_files if item.startswith("a_")]
    if len(wanted_file_list) != 1:
        report_failure("We couldn't find the correct table in the ISA object.")

    wanted_file = wanted_file_list[0]

    df = pd.read_csv(zip_file.open(wanted_file), sep = "\t", usecols = ["Sample Name"])
    sample_IDs = df["Sample Name"].tolist()

    return(sample_IDs)


def get_read_counts_from_raw_multiqc(sample_names):

    zip_file = zipfile.ZipFile(fastqc_dir + "raw_multiqc_data.zip")
    df = pd.read_csv(zip_file.open("multiqc_general_stats.txt"), sep = "\t", usecols = [0,5])
    df.columns = ["sample", "counts"]
    df.set_index("sample", inplace = True)

    return(df)


def gen_and_write_out_QC_metadata_tab(sample_names, read_counts_df):

    # generating needed order of names row writing out table (which has full file names) and as they are in the multiqc table, for finding them
    read_count_filename_list = []
    multiqc_name_list = []

    for sample in sample_names:

        read_count_filename_list.append(sample + raw_R1_suffix)
        read_count_filename_list.append(sample + raw_R2_suffix)
        multiqc_name_list.append(sample + raw_R1_suffix.replace(".fastq.gz", ""))
        multiqc_name_list.append(sample + raw_R2_suffix.replace(".fastq.gz", ""))

    # getting counts
    counts_list = []
    for entry in multiqc_name_list:

        counts_list.append(read_counts_df.at[entry, "counts"])

    # making counts output table
    out_df = pd.DataFrame()
    out_df["File Name"] = read_count_filename_list
    out_df["Number of Reads"] = counts_list

    out_df.to_csv(args.GLDS_ID + "-QC-metadata.tsv", sep = "\t", float_format='%.0f', index = False)


def get_read_count_from_df(sample_name, read_counts_tab):

    return(read_counts_tab.at[str(sample_name) + "_R1_raw", "counts"])


def gen_and_write_out_filenames_table(sample_names, read_count_tab):

    header_colnames = ["Sample Name", "Parameter Value[README]", "Term Source REF", "Term Accession Number",
                       "Parameter Value[" + raw_reads_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
                       "Parameter Value[Read Depth]", "Unit", "Term Source REF", "Term Accession Number",
                       "Parameter Value[" + filtered_reads_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
                       "Parameter Value[" + fastqc_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
                       "Parameter Value[" + assembly_based_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
                       "Parameter Value[" + assemblies_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
                       "Parameter Value[" + genes_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
                       "Parameter Value[" + annotations_and_tax_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
                       "Parameter Value[" + mapping_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
                       "Parameter Value[" + bins_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
                       "Parameter Value[" + MAGs_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
                       "Parameter Value[" + combined_output_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
                       "Parameter Value[" + read_based_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
                       "Parameter Value[Processing Info]", "Term Source REF", "Term Accession Number"]

    # entries that don't change per sample
    readme = file_prefix + "README.txt"
    fastqc = [file_prefix + "raw_multiqc_data.zip", file_prefix + "raw_multiqc_report.html", file_prefix + "filtered_multiqc_data.zip", file_prefix + "filtered_multiqc_report.html"]
    assembly_overview_tab = file_prefix + "Assembly-based-processing-overview.tsv"

    if os.path.exists(assemblies_dir + "Failed-assemblies.tsv"):
        assembly_files = [file_prefix + "assembly-summaries.tsv", file_prefix + "Failed-assemblies.tsv"]
    else:
        assembly_files = [file_prefix + "assembly-summaries.tsv"]

    if os.path.exists(bins_dir + "bins-overview.tsv"):
        bins_overview = [file_prefix + "bins-overview.tsv"]
        bins_dir_files = [file for file in os.listdir(bins_dir) if file.endswith(".fasta")]
    else:
        bins_overview = [""]

    if os.path.exists(MAGs_dir + "MAGs-overview.tsv"):
        MAGs_overview = [file_prefix + "MAGs-overview.tsv"]
        MAGs_dir_files = [file for file in os.listdir(MAGs_dir) if file.endswith(".fasta")]
    else:
        MAGs_overview = [""]

    combined_outputs = [file_prefix + "Combined-gene-level-KO-function-coverages.tsv", file_prefix + "Combined-gene-level-KO-function-coverages-CPM.tsv",
                            file_prefix + "Combined-gene-level-taxonomy-coverages.tsv", file_prefix + "Combined-gene-level-taxonomy-coverages-CPM.tsv",
                            file_prefix + "Combined-contig-level-taxonomy-coverages.tsv", file_prefix + "Combined-contig-level-taxonomy-coverages-CPM.tsv"]

    read_based_outputs = [file_prefix + "Gene-families.tsv", file_prefix + "Gene-families-grouped-by-taxa.tsv", file_prefix + "Gene-families-cpm.tsv", file_prefix + "Gene-families-KO-cpm.tsv",
                      file_prefix + "Pathway-abundances.tsv", file_prefix + "Pathway-abundances-grouped-by-taxa.tsv", file_prefix + "Pathway-abundances-cpm.tsv", file_prefix + "Pathway-coverages.tsv",
                      file_prefix + "Pathway-coverages-grouped-by-taxa.tsv", file_prefix + "Metaphlan-taxonomy.tsv"]

    processing_info = file_prefix + "processing_info.tar"

    read_count_unit = "read"
    read_count_term_source_ref = "SO"
    read_count_term_acc_number = "http://purl.obolibrary.org/obo/SO_0000150"


    building_df = pd.DataFrame(columns = header_colnames)

    for sample in sample_names:

        curr_raw_data = [file_prefix + sample + raw_R1_suffix, file_prefix + sample + raw_R2_suffix]

        curr_read_count = get_read_count_from_df(sample, read_count_tab)

        curr_filt_data = [file_prefix + sample + filtered_R1_suffix, file_prefix + sample + filtered_R2_suffix]

        # only adding file to list if it exists and isn't empty (easier for curation this way)
        curr_path = assemblies_dir + sample + assembly_suffix

        if os.path.exists(curr_path) and os.path.getsize(curr_path) > 0:
            curr_assembly = [file_prefix + sample + assembly_suffix] + assembly_files
        else:
            curr_assembly = [""]

        # only adding file to list if it exists and isn't empty (easier for curation this way)
        curr_genes = []
        for ext in ["-genes.faa", "-genes.fasta", "-genes.gff"]:
            curr_path = genes_dir + sample + ext
            
            if os.path.exists(curr_path) and os.path.getsize(curr_path) > 0:
                curr_genes.append(file_prefix + sample + ext)

        # adding empty value if all 3 missing (which i don't think happens as the gff has content either way)
        if len(curr_genes) == 0:
            curr_genes = [""]

        # these have headers even if no data for a sample, so no complications about being empty
        curr_annots = [file_prefix + sample + "-gene-coverage-annotation-and-tax.tsv", file_prefix + sample + "-contig-coverage-and-tax.tsv"]

        # only adding file to list if it exists and isn't empty (easier for curation this way)
        curr_read_mapping = []
        for ext in [".bam", "-mapping-info.txt", "-metabat-assembly-depth.tsv"]:
            curr_path = mapping_dir + sample + ext

            if os.path.exists(curr_path) and os.path.getsize(curr_path) > 0:
                curr_read_mapping.append(file_prefix + sample + ext)

        # adding empty value if all 3 missing
        if len(curr_read_mapping) == 0:
            curr_read_mapping = [""]

        if bins_overview[0] == file_prefix + "bins-overview.tsv":
            curr_bins = bins_overview + [file_prefix + file for file in bins_dir_files if file.startswith(sample)]
        else:
            curr_bins = [""]

        if MAGs_overview[0] == file_prefix + "MAGs-overview.tsv":
            curr_MAGs = MAGs_overview + [file_prefix + file for file in MAGs_dir_files if file.startswith(sample)]
        else:
            curr_MAGs = [""]

        curr_row_as_list = [sample, readme, "", "",
                            ", ".join(curr_raw_data), "", "",
                            curr_read_count, read_count_unit, read_count_term_source_ref, read_count_term_acc_number,
                            ", ".join(curr_filt_data),"", "",
                            ", ".join(fastqc), "", "",
                            assembly_overview_tab, "", "",
                            ", ".join(curr_assembly), "", "",
                            ", ".join(curr_genes), "", "",
                            ", ".join(curr_annots), "", "",
                            ", ".join(curr_read_mapping), "", "",
                            ", ".join(curr_bins), "", "",
                            ", ".join(curr_MAGs), "", "",
                            ", ".join(combined_outputs), "", "",
                            ", ".join(read_based_outputs), "", "",
                            processing_info, "", ""] 

        # adding to building dataframe
        building_df.loc[len(building_df)] = curr_row_as_list

    building_df.to_csv(args.GLDS_ID + "-associated-file-names.tsv", sep = "\t", index = False)

if __name__ == "__main__":
    main()

