#!/usr/bin/env python

"""
This is a program for generating the table needed by curation for GeneLab metagenomics processed datasets.
"""

import os
import sys
import argparse
import textwrap
import pandas as pd
import tarfile
import zipfile
import glob

parser = argparse.ArgumentParser(description="This program generates the table needed by Curation for GeneLab metagenomics \
                                             processed datasets.\
                                             Hard-coded variables that may need to be changed are at the top \
                                             of the script. It is expected to be run after `GL-validate-metagenomics` and \
                                             `GL-gen-metagenomics-readme` have been run successfully.")

required = parser.add_argument_group('required arguments')

required.add_argument("-g", "--GLDS-ID", help='GLDS ID (e.g. "GLDS-276")', action="store", required = True)
required.add_argument("-i", "--isa-zip", help='Appropriate ISA file for dataset (zipped)', action="store", required = True)
parser.add_argument("--additional-prefix", help="Add any expected additional filename prefix that was added to the files that describe multiple samples (default: \"\")", default = "", action="store")
parser.add_argument("--single-ended", help="Add this flag if data are single-end sequencing.", action="store_true")


if len(sys.argv)==1:
    parser.print_help(sys.stderr)
    sys.exit(0)

args = parser.parse_args()

additional_prefix = str(args.additional_prefix)

### hard-coded stuff we might want to change ###
file_prefix = args.GLDS_ID + "_GMetagenomics_"

raw_reads_dir = "Raw_Sequence_Data/"
fastqc_dir = "FastQC_Outputs/"
filtered_reads_dir = "Filtered_Sequence_Data/"
assembly_based_dir = "Assembly-based_Processing/"
assemblies_dir = "Assembly-based_Processing/assemblies/"
genes_dir = "Assembly-based_Processing/predicted-genes/"
annotations_and_tax_dir = "Assembly-based_Processing/annotations-and-taxonomy/"
mapping_dir = "Assembly-based_Processing/read-mapping/"
combined_output_dir = "Assembly-based_Processing/combined-outputs/"
bins_dir = "Assembly-based_Processing/bins/"
MAGs_dir = "Assembly-based_Processing/MAGs/"
read_based_dir = "Read-based_Processing/"

raw_R1_suffix_no_ext = "_R1_HRremoved_raw"
raw_R1_suffix = "_R1_HRremoved_raw.fastq.gz"
raw_R2_suffix = "_R2_HRremoved_raw.fastq.gz"
filtered_R1_suffix = "_R1_filtered.fastq.gz"
filtered_R2_suffix = "_R2_filtered.fastq.gz"
assembly_suffix = "-assembly.fasta"

# if single-end
raw_suffix = "_HRremoved_raw.fastq.gz"
raw_suffix_no_ext = "_HRremoved_raw"

filtered_suffix = "_filtered.fastq.gz"

processing_tar_file = additional_prefix + "processing_info.tar"

################################################################################

def main():

    check_for_file_and_contents(args.isa_zip)

    samples_in_ISA_order = get_samples_from_ISA(args.isa_zip)

    read_counts_df = get_read_counts_from_raw_multiqc(samples_in_ISA_order)

#    gen_and_write_out_QC_metadata_tab(samples_in_ISA_order, read_counts_df)

    gen_and_write_out_filenames_table(samples_in_ISA_order, read_counts_df, args.single_ended)

################################################################################


# setting some colors
tty_colors = {
    'green' : '\033[0;32m%s\033[0m',
    'yellow' : '\033[0;33m%s\033[0m',
    'red' : '\033[0;31m%s\033[0m'
}


### functions ###
def color_text(text, color='green'):
    if sys.stdout.isatty():
        return tty_colors[color] % text
    else:
        return text


def wprint(text):
    """ print wrapper """

    print(textwrap.fill(text, width=80, initial_indent="  ",
          subsequent_indent="  ", break_on_hyphens=False))


def report_failure(message, color = "yellow"):
    print("")
    wprint(color_text(message, color))
    print("\nJIRA-table generation failed.\n")
    sys.exit(1)


def check_for_file_and_contents(file_path):
    """ used by check_fastq_files function """

    if not os.path.exists(file_path):
        report_failure("The expected file '" + str(file_path) + "' does not exist.")
    if not os.path.getsize(file_path) > 0:
        report_failure("The file '" + str(file_path) + "' is empty.")


def get_samples_from_ISA(isa_file):
    """ gets the sample names in their order from the ISA zip file """

    zip_file = zipfile.ZipFile(isa_file)
    isa_files = zip_file.namelist()

    # getting wanted filename (those that start with "a_" seem to be what we want)
    wanted_file_list = [item for item in isa_files if item.startswith("a_")]
    if len(wanted_file_list) != 1:
        report_failure("We couldn't find the correct table in the ISA object.")

    wanted_file = wanted_file_list[0]

    df = pd.read_csv(zip_file.open(wanted_file), sep = "\t", usecols = ["Sample Name"])
    sample_IDs = df["Sample Name"].tolist()

    return(sample_IDs)


def get_read_counts_from_raw_multiqc(sample_names):

    zip_file = zipfile.ZipFile(fastqc_dir + additional_prefix + "raw_multiqc_data.zip")
    df = pd.read_csv(zip_file.open("multiqc_general_stats.txt"), sep = "\t", usecols = [0,5])
    df.columns = ["sample", "counts"]
    df.set_index("sample", inplace = True)

    return(df)


def gen_and_write_out_QC_metadata_tab(sample_names, read_counts_df):

    # generating needed order of names row writing out table (which has full file names) and as they are in the multiqc table, for finding them
    read_count_filename_list = []
    multiqc_name_list = []

    for sample in sample_names:

        read_count_filename_list.append(sample + raw_R1_suffix)
        read_count_filename_list.append(sample + raw_R2_suffix)
        multiqc_name_list.append(sample + raw_R1_suffix.replace(".fastq.gz", ""))
        multiqc_name_list.append(sample + raw_R2_suffix.replace(".fastq.gz", ""))

    # getting counts
    counts_list = []
    for entry in multiqc_name_list:

        counts_list.append(read_counts_df.at[entry, "counts"])

    # making counts output table
    out_df = pd.DataFrame()
    out_df["File Name"] = read_count_filename_list
    out_df["Number of Reads"] = counts_list

    out_df.to_csv(args.GLDS_ID + "-QC-metadata.tsv", sep = "\t", float_format='%.0f', index = False)


def get_read_count_from_df(sample_name, read_counts_tab, single_ended):

    if single_ended:

        return(read_counts_tab.at[str(sample_name) + str(raw_suffix_no_ext), "counts"])

    else:

        return(read_counts_tab.at[str(sample_name) + str(raw_R1_suffix_no_ext), "counts"])


def gen_and_write_out_filenames_table(sample_names, read_count_tab, single_ended):

    header_colnames = ["Sample Name", "Parameter Value[README]",
                       "Parameter Value[" + raw_reads_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[Read Depth]", "Unit",
                       "Parameter Value[" + filtered_reads_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + fastqc_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + assembly_based_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + assemblies_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + genes_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + annotations_and_tax_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + mapping_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + bins_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + MAGs_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + combined_output_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + read_based_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[Processing Info]"]

    # entries that don't change per sample
    readme = file_prefix + additional_prefix + "README.txt"
    fastqc = [file_prefix + additional_prefix + "raw_multiqc_data.zip", 
              file_prefix + additional_prefix + "raw_multiqc_report.html", 
              file_prefix + additional_prefix + "filtered_multiqc_data.zip", 
              file_prefix + additional_prefix + "filtered_multiqc_report.html"]

    assembly_overview_tab = file_prefix + additional_prefix + "Assembly-based-processing-overview.tsv"

    if os.path.exists(assemblies_dir + additional_prefix + "Failed-assemblies.tsv"):
        assembly_files = [file_prefix + additional_prefix + "assembly-summaries.tsv", file_prefix + additional_prefix + "Failed-assemblies.tsv"]
    else:
        assembly_files = [file_prefix + additional_prefix + "assembly-summaries.tsv"]

    if os.path.exists(bins_dir + additional_prefix + "bins-overview.tsv"):
        bins_overview = [file_prefix + additional_prefix + "bins-overview.tsv"]
        bins_dir_files = [file for file in os.listdir(bins_dir) if file.endswith(".fasta")]
    else:
        bins_overview = [""]

    if os.path.exists(MAGs_dir + additional_prefix + "MAGs-overview.tsv"):
        MAGs_overview = [file_prefix + additional_prefix + "MAGs-overview.tsv"]
        MAGs_dir_files = [file for file in os.listdir(MAGs_dir) if file.endswith(".fasta")]
    else:
        MAGs_overview = [""]

    MAG_KO_files_list = []
    if os.path.exists(MAGs_dir + additional_prefix + "MAG-level-KO-annotations.tsv"):
        MAG_KO_files_list.append(file_prefix + additional_prefix + "MAG-level-KO-annotations.tsv")
    if os.path.exists(MAGs_dir + additional_prefix + "MAG-KEGG-Decoder-out.tsv"):
        MAG_KO_files_list.append(file_prefix + additional_prefix + "MAG-KEGG-Decoder-out.tsv")
    if os.path.exists(MAGs_dir + additional_prefix + "MAG-KEGG-Decoder-out.html"):
        MAG_KO_files_list.append(file_prefix + additional_prefix + "MAG-KEGG-Decoder-out.html")


    combined_outputs = [file_prefix + additional_prefix + "Combined-gene-level-KO-function-coverages.tsv", 
                        file_prefix + additional_prefix + "Combined-gene-level-KO-function-coverages-CPM.tsv",
                        file_prefix + additional_prefix + "Combined-gene-level-taxonomy-coverages.tsv", 
                        file_prefix + additional_prefix + "Combined-gene-level-taxonomy-coverages-CPM.tsv",
                        file_prefix + additional_prefix + "Combined-contig-level-taxonomy-coverages.tsv", 
                        file_prefix + additional_prefix + "Combined-contig-level-taxonomy-coverages-CPM.tsv"]

    read_based_outputs = [file_prefix + additional_prefix + "Gene-families.tsv", 
                          file_prefix + additional_prefix + "Gene-families-grouped-by-taxa.tsv", 
                          file_prefix + additional_prefix + "Gene-families-cpm.tsv", 
                          file_prefix + additional_prefix + "Gene-families-KO-cpm.tsv",
                          file_prefix + additional_prefix + "Pathway-abundances.tsv", 
                          file_prefix + additional_prefix + "Pathway-abundances-grouped-by-taxa.tsv", 
                          file_prefix + additional_prefix + "Pathway-abundances-cpm.tsv", 
                          file_prefix + additional_prefix + "Pathway-coverages.tsv",
                          file_prefix + additional_prefix + "Pathway-coverages-grouped-by-taxa.tsv", 
                          file_prefix + additional_prefix + "Metaphlan-taxonomy.tsv"]

    processing_info = file_prefix + additional_prefix + "processing_info.tar"

    read_count_unit = "read"

    building_df = pd.DataFrame(columns = header_colnames)

    for sample in sample_names:

        if single_ended:

            curr_raw_data = [file_prefix + sample + raw_suffix]

        else:

            curr_raw_data = [file_prefix + sample + raw_R1_suffix, file_prefix + sample + raw_R2_suffix]


        curr_read_count = get_read_count_from_df(sample, read_count_tab, single_ended)

        if single_ended:

            curr_filt_data = [file_prefix + sample + filtered_suffix]

        else:

            curr_filt_data = [file_prefix + sample + filtered_R1_suffix, file_prefix + sample + filtered_R2_suffix]


        # only adding file to list if it exists and isn't empty (easier for curation this way)
        curr_path = assemblies_dir + sample + assembly_suffix

        if os.path.exists(curr_path) and os.path.getsize(curr_path) > 0:
            curr_assembly = [file_prefix + sample + assembly_suffix] + assembly_files
        else:
            curr_assembly = [""]

        # only adding file to list if it exists and isn't empty (easier for curation this way)
        curr_genes = []
        for ext in ["-genes.faa", "-genes.fasta", "-genes.gff"]:
            curr_path = genes_dir + sample + ext
            
            if os.path.exists(curr_path) and os.path.getsize(curr_path) > 0:
                curr_genes.append(file_prefix + sample + ext)

        # adding empty value if all 3 missing (which i don't think happens as the gff has content either way)
        if len(curr_genes) == 0:
            curr_genes = [""]

        # these have headers even if no data for a sample, so no complications about being empty
        curr_annots = [file_prefix + sample + "-gene-coverage-annotation-and-tax.tsv", file_prefix + sample + "-contig-coverage-and-tax.tsv"]

        # only adding file to list if it exists and isn't empty (easier for curation this way)
        curr_read_mapping = []
        for ext in [".bam", "-mapping-info.txt", "-metabat-assembly-depth.tsv"]:
            curr_path = mapping_dir + sample + ext

            if os.path.exists(curr_path) and os.path.getsize(curr_path) > 0:
                curr_read_mapping.append(file_prefix + sample + ext)

        # adding empty value if all 3 missing
        if len(curr_read_mapping) == 0:
            curr_read_mapping = [""]

        if bins_overview[0] == file_prefix + additional_prefix + "bins-overview.tsv":
            curr_bins = bins_overview + [file_prefix + file for file in bins_dir_files if file.startswith(sample)]
        else:
            curr_bins = [""]

        if MAGs_overview[0] == file_prefix + additional_prefix + "MAGs-overview.tsv":
            curr_MAGs = MAGs_overview + [file_prefix + file for file in MAGs_dir_files if file.startswith(sample)] + MAG_KO_files_list
        else:
            curr_MAGs = [""]

        curr_row_as_list = [sample, readme,
                            ", ".join(curr_raw_data),
                            curr_read_count, read_count_unit,
                            ", ".join(curr_filt_data),
                            ", ".join(fastqc),
                            assembly_overview_tab,
                            ", ".join(curr_assembly),
                            ", ".join(curr_genes),
                            ", ".join(curr_annots),
                            ", ".join(curr_read_mapping),
                            ", ".join(curr_bins),
                            ", ".join(curr_MAGs),
                            ", ".join(combined_outputs),
                            ", ".join(read_based_outputs),
                            processing_info] 

        # adding to building dataframe
        building_df.loc[len(building_df)] = curr_row_as_list

    if additional_prefix != "":
        building_df.to_csv(str(args.GLDS_ID) + "_" + additional_prefix + "associated-file-names.tsv", sep = "\t", index = False)
    else:
        building_df.to_csv(str(args.GLDS_ID) + "-associated-file-names.tsv", sep = "\t", index = False)


### commented out version below was when Curation wanted me to include "Term Source REF" and "Term Accession Number" columns
# def gen_and_write_out_filenames_table(sample_names, read_count_tab):

#     header_colnames = ["Sample Name", "Parameter Value[README]", "Term Source REF", "Term Accession Number",
#                        "Parameter Value[" + raw_reads_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                        "Parameter Value[Read Depth]", "Unit", "Term Source REF", "Term Accession Number",
#                        "Parameter Value[" + filtered_reads_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                        "Parameter Value[" + fastqc_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                        "Parameter Value[" + assembly_based_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                        "Parameter Value[" + assemblies_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                        "Parameter Value[" + genes_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                        "Parameter Value[" + annotations_and_tax_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                        "Parameter Value[" + mapping_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                        "Parameter Value[" + bins_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                        "Parameter Value[" + MAGs_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                        "Parameter Value[" + combined_output_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                        "Parameter Value[" + read_based_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                        "Parameter Value[Processing Info]", "Term Source REF", "Term Accession Number"]

#     # entries that don't change per sample
#     readme = file_prefix + additional_prefix + "README.txt"
#     fastqc = [file_prefix + additional_prefix + "raw_multiqc_data.zip", 
#               file_prefix + additional_prefix + "raw_multiqc_report.html", 
#               file_prefix + additional_prefix + "filtered_multiqc_data.zip", 
#               file_prefix + additional_prefix + "filtered_multiqc_report.html"]

#     assembly_overview_tab = file_prefix + additional_prefix + "Assembly-based-processing-overview.tsv"

#     if os.path.exists(assemblies_dir + additional_prefix + "Failed-assemblies.tsv"):
#         assembly_files = [file_prefix + additional_prefix + "assembly-summaries.tsv", file_prefix + additional_prefix + "Failed-assemblies.tsv"]
#     else:
#         assembly_files = [file_prefix + additional_prefix + "assembly-summaries.tsv"]

#     if os.path.exists(bins_dir + additional_prefix + "bins-overview.tsv"):
#         bins_overview = [file_prefix + additional_prefix + "bins-overview.tsv"]
#         bins_dir_files = [file for file in os.listdir(bins_dir) if file.endswith(".fasta")]
#     else:
#         bins_overview = [""]

#     if os.path.exists(MAGs_dir + additional_prefix + "MAGs-overview.tsv"):
#         MAGs_overview = [file_prefix + additional_prefix + "MAGs-overview.tsv"]
#         MAGs_dir_files = [file for file in os.listdir(MAGs_dir) if file.endswith(".fasta")]
#     else:
#         MAGs_overview = [""]

#     MAG_KO_files_list = []
#     if os.path.exists(MAGs_dir + additional_prefix + "MAG-level-KO-annotations.tsv"):
#         MAG_KO_files_list.append(file_prefix + additional_prefix + "MAG-level-KO-annotations.tsv")
#     if os.path.exists(MAGs_dir + additional_prefix + "MAG-KEGG-Decoder-out.tsv"):
#         MAG_KO_files_list.append(file_prefix + additional_prefix + "MAG-KEGG-Decoder-out.tsv")
#     if os.path.exists(MAGs_dir + additional_prefix + "MAG-KEGG-Decoder-out.html"):
#         MAG_KO_files_list.append(file_prefix + additional_prefix + "MAG-KEGG-Decoder-out.html")


#     combined_outputs = [file_prefix + additional_prefix + "Combined-gene-level-KO-function-coverages.tsv", 
#                         file_prefix + additional_prefix + "Combined-gene-level-KO-function-coverages-CPM.tsv",
#                         file_prefix + additional_prefix + "Combined-gene-level-taxonomy-coverages.tsv", 
#                         file_prefix + additional_prefix + "Combined-gene-level-taxonomy-coverages-CPM.tsv",
#                         file_prefix + additional_prefix + "Combined-contig-level-taxonomy-coverages.tsv", 
#                         file_prefix + additional_prefix + "Combined-contig-level-taxonomy-coverages-CPM.tsv"]

#     read_based_outputs = [file_prefix + additional_prefix + "Gene-families.tsv", 
#                           file_prefix + additional_prefix + "Gene-families-grouped-by-taxa.tsv", 
#                           file_prefix + additional_prefix + "Gene-families-cpm.tsv", 
#                           file_prefix + additional_prefix + "Gene-families-KO-cpm.tsv",
#                           file_prefix + additional_prefix + "Pathway-abundances.tsv", 
#                           file_prefix + additional_prefix + "Pathway-abundances-grouped-by-taxa.tsv", 
#                           file_prefix + additional_prefix + "Pathway-abundances-cpm.tsv", 
#                           file_prefix + additional_prefix + "Pathway-coverages.tsv",
#                           file_prefix + additional_prefix + "Pathway-coverages-grouped-by-taxa.tsv", 
#                           file_prefix + additional_prefix + "Metaphlan-taxonomy.tsv"]

#     processing_info = file_prefix + additional_prefix + "processing_info.tar"

#     read_count_unit = "read"
#     read_count_term_source_ref = "SO"
#     read_count_term_acc_number = "http://purl.obolibrary.org/obo/SO_0000150"


#     building_df = pd.DataFrame(columns = header_colnames)

#     for sample in sample_names:

#         curr_raw_data = [file_prefix + sample + raw_R1_suffix, file_prefix + sample + raw_R2_suffix]

#         curr_read_count = get_read_count_from_df(sample, read_count_tab)

#         curr_filt_data = [file_prefix + sample + filtered_R1_suffix, file_prefix + sample + filtered_R2_suffix]

#         # only adding file to list if it exists and isn't empty (easier for curation this way)
#         curr_path = assemblies_dir + sample + assembly_suffix

#         if os.path.exists(curr_path) and os.path.getsize(curr_path) > 0:
#             curr_assembly = [file_prefix + sample + assembly_suffix] + assembly_files
#         else:
#             curr_assembly = [""]

#         # only adding file to list if it exists and isn't empty (easier for curation this way)
#         curr_genes = []
#         for ext in ["-genes.faa", "-genes.fasta", "-genes.gff"]:
#             curr_path = genes_dir + sample + ext
            
#             if os.path.exists(curr_path) and os.path.getsize(curr_path) > 0:
#                 curr_genes.append(file_prefix + sample + ext)

#         # adding empty value if all 3 missing (which i don't think happens as the gff has content either way)
#         if len(curr_genes) == 0:
#             curr_genes = [""]

#         # these have headers even if no data for a sample, so no complications about being empty
#         curr_annots = [file_prefix + sample + "-gene-coverage-annotation-and-tax.tsv", file_prefix + sample + "-contig-coverage-and-tax.tsv"]

#         # only adding file to list if it exists and isn't empty (easier for curation this way)
#         curr_read_mapping = []
#         for ext in [".bam", "-mapping-info.txt", "-metabat-assembly-depth.tsv"]:
#             curr_path = mapping_dir + sample + ext

#             if os.path.exists(curr_path) and os.path.getsize(curr_path) > 0:
#                 curr_read_mapping.append(file_prefix + sample + ext)

#         # adding empty value if all 3 missing
#         if len(curr_read_mapping) == 0:
#             curr_read_mapping = [""]

#         if bins_overview[0] == file_prefix + additional_prefix + "bins-overview.tsv":
#             curr_bins = bins_overview + [file_prefix + file for file in bins_dir_files if file.startswith(sample)]
#         else:
#             curr_bins = [""]

#         if MAGs_overview[0] == file_prefix + additional_prefix + "MAGs-overview.tsv":
#             curr_MAGs = MAGs_overview + [file_prefix + file for file in MAGs_dir_files if file.startswith(sample)] + MAG_KO_files_list
#         else:
#             curr_MAGs = [""]

#         curr_row_as_list = [sample, readme, "", "",
#                             ", ".join(curr_raw_data), "", "",
#                             curr_read_count, read_count_unit, read_count_term_source_ref, read_count_term_acc_number,
#                             ", ".join(curr_filt_data),"", "",
#                             ", ".join(fastqc), "", "",
#                             assembly_overview_tab, "", "",
#                             ", ".join(curr_assembly), "", "",
#                             ", ".join(curr_genes), "", "",
#                             ", ".join(curr_annots), "", "",
#                             ", ".join(curr_read_mapping), "", "",
#                             ", ".join(curr_bins), "", "",
#                             ", ".join(curr_MAGs), "", "",
#                             ", ".join(combined_outputs), "", "",
#                             ", ".join(read_based_outputs), "", "",
#                             processing_info, "", ""] 

#         # adding to building dataframe
#         building_df.loc[len(building_df)] = curr_row_as_list

#     if additional_prefix != "":
#         building_df.to_csv(str(args.GLDS_ID) + "_" + additional_prefix + "associated-file-names.tsv", sep = "\t", index = False)
#     else:
#         building_df.to_csv(str(args.GLDS_ID) + "-associated-file-names.tsv", sep = "\t", index = False)

if __name__ == "__main__":
    main()

