#!/usr/bin/env python

"""
This is a program for generating the file-associations table needed by Curation for newly processed datasets.
"""

import os
import sys
import argparse
import textwrap
import pandas as pd
import zipfile
import glob

parser = argparse.ArgumentParser(description = "This program generates the file-assocations table needed by Curation for \
                                               newly processed datasets, currently designed to work for metagenomics and \
                                               amplicon datasets. It is intended to be run after `GL-validate-processed-data` \
                                               has been run successfully.")

required = parser.add_argument_group('required arguments')

required.add_argument("-a", "--assay", choices = ['Amplicon', 'Metagenomics'], 
                      help = "Specifies which datatype (assay) this is for", action = "store", required = True)

required.add_argument("-g", "--GLDS-ID", help = 'GLDS ID (e.g. "GLDS-276")', action = "store", required = True)
parser.add_argument("-i", "--isa-zip", help = 'Appropriate ISA file for dataset (a zip archive, providing this will assume there is only one a_* assay table in there, \
                                               if that\'s not the case, explicitly provide the assay table to the \'-a\' argument instead)', action = "store", default = "")
parser.add_argument("--assay-table", help = 'Appropriate assay table for dataset (this can be provided directly instead of being pulled from an ISA object)', action = "store", default = "")
parser.add_argument("--primers-already-trimmed", help="Add this flag if primers were trimmed prior to GeneLab processing, \
                    therefore there are no trimmed sequence data.", action = "store_true")
parser.add_argument("--single-ended", help = "Add this flag if data are single-end sequencing.", action = "store_true")
parser.add_argument("--type", help = 'Specify if ASVs or OTUs (default: "ASVs"; only relevant for Amplicon)', action = "store", choices = ["ASVs", "OTUs"], default = "ASVs")
parser.add_argument("--map", help = 'Mapping file if samples come from more than one primer set (only relevant for Amplicon; tab-delimited, first column holds sample IDs, \
                                    second column holds the filename prefix of the outputs specific to that sample)', action = "store")
parser.add_argument("-p", "--output-prefix", help = "Output additional file prefix if there is one", action = "store", default = "")
parser.add_argument("--R1-used-as-single-ended-data", help = "Provide this flag if processing only R1 reads as single-end (as the expected raw filename suffixes will have 'R1' in there)", 
                    action = "store_true")


if len(sys.argv)==1:
    parser.print_help(sys.stderr)
    sys.exit(0)

args = parser.parse_args()


################################################################################

def main():

    preflight_checks()

    assay_table = get_assay_table()

    sample_names, unique_filename_prefixes = get_sample_names_and_unique_filenames(assay_table)

    sample_file_dict = dict(zip(unique_filename_prefixes, sample_names))

    if args.map:
        map_tab = pd.read_csv(args.map, sep = "\t", names = ["sample", "prefix"])
        map_tab.set_index("sample", inplace = True)

    else:
        map_tab = None

    read_counts_df = get_read_counts_from_raw_multiqc(sample_names, map_tab)

    if args.assay == "Amplicon":

        gen_and_write_out_amplicon_filenames_table(unique_filename_prefixes, map_tab, read_counts_df, sample_file_dict)

    elif args.assay == "Metagenomics":

        gen_and_write_out_metagenomics_filenames_table(unique_filename_prefixes, map_tab, read_counts_df, sample_file_dict)


################################################################################


# setting some colors
tty_colors = {
    'green' : '\033[0;32m%s\033[0m',
    'yellow' : '\033[0;33m%s\033[0m',
    'red' : '\033[0;31m%s\033[0m'
}


### functions ###
def color_text(text, color='green'):
    if sys.stdout.isatty():
        return tty_colors[color] % text
    else:
        return text


def wprint(text):
    """ print wrapper """

    print(textwrap.fill(text, width=80, initial_indent="  ", 
          subsequent_indent="  ", break_on_hyphens=False))


def report_failure(message, color = "yellow"):
    print("")
    wprint(color_text(message, color))
    print("\nCuration file-associations table generation failed.\n")
    sys.exit(1)


def preflight_checks():

    # ensuring at least, and only, one of input ISA or assay table
    if args.isa_zip == "" and args.assay_table == "":
        report_failure("This program requires either an input ISA object (passed to '-i') or a specific assay table (passed to '-a').")

    if args.isa_zip != "" and args.assay_table != "":
        report_failure("This program requires *only* an input ISA object (passed to '-i') or a specific assay table (passed to '-a'), not both.")


def check_for_file_and_contents(file_path):

    if not os.path.exists(file_path):
        report_failure("The expected file '" + str(file_path) + "' does not exist.")
    if not os.path.getsize(file_path) > 0:
        report_failure("The file '" + str(file_path) + "' is empty.")


def get_assay_table_from_ISA(isa_file):
    """ tries to find a single assay table in an isa object """

    zip_file = zipfile.ZipFile(isa_file)
    isa_files = zip_file.namelist()

    # getting wanted filename (those that start with "a_" seem to be what we want)
    wanted_file_list = [item for item in isa_files if item.startswith("a_")]
    if len(wanted_file_list) != 1:
        report_failure("We couldn't find the correct assay table in the ISA object, consider passing it directly to the '-a' argument.")

    wanted_file = wanted_file_list[0]

    df = pd.read_csv(zip_file.open(wanted_file), sep = "\t")

    return(df)


def get_assay_table():
    """ returns assay table whether provided directory or pulled from ISA archive """

    # getting assay table if we are using an input isa object
    if args.isa_zip != "":
        check_for_file_and_contents(args.isa_zip)

        assay_table = get_assay_table_from_ISA(args.isa_zip)

    # reading assay table if provided directly
    else:

        check_for_file_and_contents(args.assay_table)
        assay_table = pd.read_csv(args.assay_table, sep = "\t")

    return(assay_table)


def get_sample_names_and_unique_filenames(assay_table):
    """
    This gets the sample names (first column) from the assay table,
    and tries to get what would have been the unique filename prefixes generated from those
    based on what's in the Raw Data File column of the assay table.
    """

    sample_names = assay_table["Sample Name"].tolist()

    all_filenames = assay_table["Raw Data File"]

    unique_filename_prefixes = []
    
    # attempting to split if they have multiple files (like paired-end)
    # and also removing the common prefixes and suffixes intending to create the same 
    # unique filenames used for processing

    if args.assay == "Amplicon":

        for entry in all_filenames:

            # splitting if there are more than one (like with paired-end)
            curr_name = entry.split(",")[0]

            # removing expected prefix
            curr_name = curr_name.replace(raw_file_prefix, "")

            # removing potential suffixes (also checking R2 in case they are not in the appropriate order in the sample table, e.g. R2 before R1)
            curr_name = curr_name.replace(raw_R1_suffix, "")
            curr_name = curr_name.replace(raw_R2_suffix, "")
            curr_name = curr_name.replace(raw_suffix, "")

            unique_filename_prefixes.append(curr_name)

    # metagenomics will somethings have _HRremoved_ in the suffix and sometimes not in the assay table (for older datasets)
    # so dealing with that here
    elif args.assay == "Metagenomics":

        # getting one file name so we can figure out which suffix is there
        one_entry = all_filenames[0].split(",")[0]

        if args.single_ended:

            if one_entry.endswith(older_raw_suffix):

                raw_suffix = older_raw_suffix

        else:

            if one_entry.endswith(older_raw_R1_suffix):

                raw_R1_suffix = older_raw_R1_suffix
                raw_R2_suffix = older_raw_R2_suffix


        for entry in all_filenames:

            # splitting if there are more than one (like with paired-end)
            curr_name = entry.split(",")[0]

            # removing expected prefix
            curr_name = curr_name.replace(raw_file_prefix, "")

            # removing potential suffixes (also checking R2 in case they are not in the appropriate order in the sample table, e.g. R2 before R1)
            curr_name = curr_name.replace(raw_R1_suffix, "")
            curr_name = curr_name.replace(raw_R2_suffix, "")

            if args.single_ended:

                curr_name = curr_name.replace(raw_suffix, "")

            unique_filename_prefixes.append(curr_name)

    return(sample_names, unique_filename_prefixes)


def get_samples_from_ISA(isa_file):
    """ gets the sample names in their order from the ISA zip file """

    zip_file = zipfile.ZipFile(isa_file)
    isa_files = zip_file.namelist()

    # getting wanted filename (those that start with "a_" seem to be what we want)
    wanted_file_list = [item for item in isa_files if item.startswith("a_")]
    if len(wanted_file_list) != 1:
        report_failure("We couldn't find the correct assay table in the ISA object, consider passing it directly to the '-a' argument.")

    wanted_file = wanted_file_list[0]

    df = pd.read_csv(zip_file.open(wanted_file), sep = "\t", usecols = ["Sample Name"])
    sample_IDs = df["Sample Name"].tolist()

    return(sample_IDs)


def get_read_counts_from_raw_multiqc(sample_names, mapping_tab):

    # these are in multiple files if there was a mapping input table
    if isinstance(mapping_tab, pd.DataFrame):
        unique_prefixes = mapping_tab.prefix.unique()

        # starting list to hold all dataframes we'll read in
        df_list = []

        # working through each one
        for prefix in unique_prefixes:

            curr_file_path = fastqc_dir + output_prefix + prefix + "-raw_multiqc_data.zip"

            # making sure there are multiqc files for each unique prefix given in the mapping table
            check_for_file_and_contents(curr_file_path)

            # reading in
            zip_file = zipfile.ZipFile(curr_file_path)
            curr_df = pd.read_csv(zip_file.open("multiqc_general_stats.txt"), sep = "\t", usecols = [0,5])
            curr_df.columns = ["sample", "counts"]
            curr_df.set_index("sample", inplace = True)

            # adding to list
            df_list.append(curr_df)

        # combining tables
        df = pd.concat(df_list, axis = 0)

        return(df)

    else:
        zip_file = zipfile.ZipFile(fastqc_dir + output_prefix + "raw_multiqc_data.zip")
        df = pd.read_csv(zip_file.open("multiqc_general_stats.txt"), sep = "\t", usecols = [0,5])
        df.columns = ["sample", "counts"]
        df.set_index("sample", inplace = True)

        return(df)


def get_prefix_from_map(sample_name, mapping_tab):

    # returning empty string if no mapping
    if not isinstance(mapping_tab, pd.DataFrame):
        return("")
    else:
        return(mapping_tab.at[sample_name, "prefix"] + "-")


def get_read_count_from_df(sample_name, read_counts_tab):

    if args.single_ended:

        return(round(read_counts_tab.at[str(sample_name) + raw_suffix.replace(".fastq.gz", ""), "counts"]))

    else:

        return(round(read_counts_tab.at[str(sample_name) + raw_R1_suffix.replace(".fastq.gz", ""), "counts"]))


def gen_and_write_out_amplicon_filenames_table(unique_filename_prefixes, mapping_tab, read_count_tab, sample_file_dict):

    ## builds as if primers were trimmed by the workflow (with Trimmed column), but that is removed later if
    ## --primers-already-trimmed argument was provided
    header_colnames = ["Sample Name", 
                       "Parameter Value[README]",
                       "Parameter Value[" + raw_reads_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[Read Depth]", 
                       "Unit",
                       "Parameter Value[" + trimmed_reads_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + filtered_reads_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + fastqc_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + final_outputs_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[Processing Info]"]


    # entries and values that don't change per sample
    readme = file_prefix + output_prefix + "README.txt"
    fastqc = [file_prefix + output_prefix + "raw_multiqc_data.zip", 
              file_prefix + output_prefix + "raw_multiqc_report.html", 
              file_prefix + output_prefix + "filtered_multiqc_data.zip", 
              file_prefix + output_prefix + "filtered_multiqc_report.html"]
    
    if args.type == "ASVs":
        rep_seq_output = file_prefix + output_prefix + "ASVs.fasta"
    else:
        rep_seq_output = file_prefix + output_prefix + "OTUs.fasta"

    final_outputs = [rep_seq_output, 
                     file_prefix + output_prefix + "counts.tsv", 
                     file_prefix + output_prefix + "read-count-tracking.tsv", 
                     file_prefix + output_prefix + "taxonomy-and-counts.biom.zip", 
                     file_prefix + output_prefix + "taxonomy-and-counts.tsv", 
                     file_prefix + output_prefix + "taxonomy.tsv"]

    processing_info = file_prefix + output_prefix + "processing_info.zip"

    read_count_unit = "read"
    read_count_term_source_ref = "SO"
    read_count_term_acc_number = "http://purl.obolibrary.org/obo/SO_0000150"

    building_df = pd.DataFrame(columns = header_colnames)

    for sample in unique_filename_prefixes:

        if args.single_ended:

            # if only forward read was used, still want to include both in the "Raw Data" column because this is tied to the hosted raw data, not just what was used here
            if args.R1_used_as_single_ended_data:
                curr_raw_data = [raw_file_prefix + sample + raw_R1_suffix, raw_file_prefix + sample + raw_R2_suffix]

            else:
                curr_raw_data = [raw_file_prefix + sample + raw_suffix]

            curr_trimmed_data = [file_prefix + sample + primer_trimmed_suffix, 
                                 file_prefix + output_prefix + "trimmed-read-counts.tsv", 
                                 file_prefix + output_prefix + "cutadapt.log"]

            curr_filt_data = [file_prefix + sample + filtered_suffix, 
                              file_prefix + output_prefix + "filtered-read-counts.tsv"]


        else:

            curr_raw_data = [raw_file_prefix + sample + raw_R1_suffix, raw_file_prefix + sample + raw_R2_suffix]
            curr_trimmed_data = [file_prefix + sample + primer_trimmed_R1_suffix, 
                                 file_prefix + sample + primer_trimmed_R2_suffix, 
                                 file_prefix + output_prefix + "trimmed-read-counts.tsv", 
                                 file_prefix + output_prefix + "cutadapt.log"]
            curr_filt_data = [file_prefix + sample + filtered_R1_suffix, 
                              file_prefix + sample + filtered_R2_suffix, 
                              file_prefix + output_prefix + "filtered-read-counts.tsv"]

        curr_read_count = get_read_count_from_df(sample, read_count_tab)

        curr_row_as_list = [sample_file_dict[sample],
                            readme,
                            ", ".join(curr_raw_data),
                            curr_read_count, 
                            read_count_unit,
                            ", ".join(curr_trimmed_data),
                            ", ".join(curr_filt_data),
                            ", ".join(fastqc),
                            ", ".join(final_outputs),
                            processing_info]

        # adding to building dataframe
        building_df.loc[len(building_df)] = curr_row_as_list

    # removing trimmed column if primers were already removed and no primer-removal was done
    if args.primers_already_trimmed:

        building_df.drop("Parameter Value[Trimmed Sequence Data]", axis = 1, inplace = True)

    # writing out
    if output_prefix != "":
        building_df.to_csv(str(args.GLDS_ID) + "_" + output_prefix + "associated-file-names.tsv", sep = "\t", index = False)
    else:
        building_df.to_csv(str(args.GLDS_ID) + "-associated-file-names.tsv", sep = "\t", index = False)


def gen_and_write_out_metagenomics_filenames_table(unique_filename_prefixes, mapping_tab, read_count_tab, sample_file_dict):

    header_colnames = ["Sample Name", "Parameter Value[README]",
                       "Parameter Value[" + raw_reads_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[Read Depth]", "Unit",
                       "Parameter Value[" + filtered_reads_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + fastqc_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + assembly_based_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + assemblies_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + genes_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + annotations_and_tax_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + mapping_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + bins_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + MAGs_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + combined_output_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + read_based_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[Processing Info]"]

    # entries that don't change per sample
    readme = file_prefix + output_prefix + "README.txt"
    fastqc = [file_prefix + output_prefix + "raw_multiqc_data.zip", 
              file_prefix + output_prefix + "raw_multiqc_report.html", 
              file_prefix + output_prefix + "filtered_multiqc_data.zip", 
              file_prefix + output_prefix + "filtered_multiqc_report.html"]

    assembly_overview_tab = file_prefix + output_prefix + "Assembly-based-processing-overview.tsv"

    if os.path.exists(assemblies_dir + output_prefix + "Failed-assemblies.tsv"):
        assembly_files = [file_prefix + output_prefix + "assembly-summaries.tsv", file_prefix + output_prefix + "Failed-assemblies.tsv"]
    else:
        assembly_files = [file_prefix + output_prefix + "assembly-summaries.tsv"]

    if os.path.exists(bins_dir + output_prefix + "bins-overview.tsv"):
        bins_overview = [file_prefix + output_prefix + "bins-overview.tsv"]
        bins_dir_files = [file for file in os.listdir(bins_dir) if file.endswith(".fasta")]
    else:
        bins_overview = [""]

    if os.path.exists(MAGs_dir + output_prefix + "MAGs-overview.tsv"):
        MAGs_overview = [file_prefix + output_prefix + "MAGs-overview.tsv"]
        MAGs_dir_files = [file for file in os.listdir(MAGs_dir) if file.endswith(".fasta")]
    else:
        MAGs_overview = [""]

    MAG_KO_files_list = []
    if os.path.exists(MAGs_dir + output_prefix + "MAG-level-KO-annotations.tsv"):
        MAG_KO_files_list.append(file_prefix + output_prefix + "MAG-level-KO-annotations.tsv")
    if os.path.exists(MAGs_dir + output_prefix + "MAG-KEGG-Decoder-out.tsv"):
        MAG_KO_files_list.append(file_prefix + output_prefix + "MAG-KEGG-Decoder-out.tsv")
    if os.path.exists(MAGs_dir + output_prefix + "MAG-KEGG-Decoder-out.html"):
        MAG_KO_files_list.append(file_prefix + output_prefix + "MAG-KEGG-Decoder-out.html")


    combined_outputs = [file_prefix + output_prefix + "Combined-gene-level-KO-function-coverages.tsv", 
                        file_prefix + output_prefix + "Combined-gene-level-KO-function-coverages-CPM.tsv",
                        file_prefix + output_prefix + "Combined-gene-level-taxonomy-coverages.tsv", 
                        file_prefix + output_prefix + "Combined-gene-level-taxonomy-coverages-CPM.tsv",
                        file_prefix + output_prefix + "Combined-contig-level-taxonomy-coverages.tsv", 
                        file_prefix + output_prefix + "Combined-contig-level-taxonomy-coverages-CPM.tsv"]

    read_based_outputs = [file_prefix + output_prefix + "Gene-families.tsv", 
                          file_prefix + output_prefix + "Gene-families-grouped-by-taxa.tsv", 
                          file_prefix + output_prefix + "Gene-families-cpm.tsv", 
                          file_prefix + output_prefix + "Gene-families-KO-cpm.tsv",
                          file_prefix + output_prefix + "Pathway-abundances.tsv", 
                          file_prefix + output_prefix + "Pathway-abundances-grouped-by-taxa.tsv", 
                          file_prefix + output_prefix + "Pathway-abundances-cpm.tsv", 
                          file_prefix + output_prefix + "Pathway-coverages.tsv",
                          file_prefix + output_prefix + "Pathway-coverages-grouped-by-taxa.tsv", 
                          file_prefix + output_prefix + "Metaphlan-taxonomy.tsv"]

    processing_info = file_prefix + output_prefix + "processing_info.zip"

    read_count_unit = "read"

    building_df = pd.DataFrame(columns = header_colnames)

    for sample in unique_filename_prefixes:

        if args.single_ended:

            curr_raw_data = [raw_file_prefix + sample + raw_suffix]
            curr_filt_data = [file_prefix + sample + filtered_suffix]

        else:

            curr_raw_data = [raw_file_prefix + sample + raw_R1_suffix, raw_file_prefix + sample + raw_R2_suffix]
            curr_filt_data = [file_prefix + sample + filtered_R1_suffix, file_prefix + sample + filtered_R2_suffix]

        curr_read_count = get_read_count_from_df(sample, read_count_tab)

        # only adding file to list if it exists and isn't empty (easier for curation this way)
        curr_path = assemblies_dir + sample + assembly_suffix

        if os.path.exists(curr_path) and os.path.getsize(curr_path) > 0:
            curr_assembly = [file_prefix + sample + assembly_suffix] + assembly_files
        else:
            curr_assembly = [""]

        # only adding file to list if it exists and isn't empty (easier for curation this way)
        curr_genes = []
        for ext in ["-genes.faa", "-genes.fasta", "-genes.gff"]:
            curr_path = genes_dir + sample + ext
            
            if os.path.exists(curr_path) and os.path.getsize(curr_path) > 0:
                curr_genes.append(file_prefix + sample + ext)

        # adding empty value if all 3 missing (which i don't think happens as the gff has content either way)
        if len(curr_genes) == 0:
            curr_genes = [""]

        # these have headers even if no data for a sample, so no complications about being empty
        curr_annots = [file_prefix + sample + "-gene-coverage-annotation-and-tax.tsv", file_prefix + sample + "-contig-coverage-and-tax.tsv"]

        # only adding file to list if it exists and isn't empty (easier for curation this way)
        curr_read_mapping = []
        for ext in [".bam", "-mapping-info.txt", "-metabat-assembly-depth.tsv"]:
            curr_path = mapping_dir + sample + ext

            if os.path.exists(curr_path) and os.path.getsize(curr_path) > 0:
                curr_read_mapping.append(file_prefix + sample + ext)

        # adding empty value if all 3 missing
        if len(curr_read_mapping) == 0:
            curr_read_mapping = [""]

        if bins_overview[0] == file_prefix + output_prefix + "bins-overview.tsv":
            curr_bins = bins_overview + [file_prefix + file for file in bins_dir_files if file.startswith(sample)]
        else:
            curr_bins = [""]

        if MAGs_overview[0] == file_prefix + output_prefix + "MAGs-overview.tsv":
            curr_MAGs = MAGs_overview + [file_prefix + file for file in MAGs_dir_files if file.startswith(sample)] + MAG_KO_files_list
        else:
            curr_MAGs = [""]

        curr_row_as_list = [sample_file_dict[sample],
                            readme,
                            ", ".join(curr_raw_data),
                            curr_read_count, read_count_unit,
                            ", ".join(curr_filt_data),
                            ", ".join(fastqc),
                            assembly_overview_tab,
                            ", ".join(curr_assembly),
                            ", ".join(curr_genes),
                            ", ".join(curr_annots),
                            ", ".join(curr_read_mapping),
                            ", ".join(curr_bins),
                            ", ".join(curr_MAGs),
                            ", ".join(combined_outputs),
                            ", ".join(read_based_outputs),
                            processing_info] 

        # adding to building dataframe
        building_df.loc[len(building_df)] = curr_row_as_list

    if output_prefix != "":
        building_df.to_csv(str(args.GLDS_ID) + "_" + output_prefix + "associated-file-names.tsv", sep = "\t", index = False)
    else:
        building_df.to_csv(str(args.GLDS_ID) + "-associated-file-names.tsv", sep = "\t", index = False)


### variable setup ###

# universal settings
output_prefix = str(args.output_prefix)
fastqc_dir = "FastQC_Outputs/"
raw_reads_dir = "Raw_Sequence_Data/"
filtered_reads_dir = "Filtered_Sequence_Data/"
filtered_suffix = "_filtered.fastq.gz"
filtered_R1_suffix = "_R1_filtered.fastq.gz"
filtered_R2_suffix = "_R2_filtered.fastq.gz"


if args.assay == "Amplicon":

    file_prefix = args.GLDS_ID + "_GAmplicon_"

    # this one is only used for the raw data files
    raw_file_prefix = args.GLDS_ID + "_Amplicon_"

    raw_suffix = "_raw.fastq.gz"
    trimmed_reads_dir = "Trimmed_Sequence_Data/"
    final_outputs_dir = "Final_Outputs/"

    raw_R1_suffix = "_R1_raw.fastq.gz"
    raw_R2_suffix = "_R2_raw.fastq.gz"
    if args.R1_used_as_single_ended_data:
        raw_suffix = raw_R1_suffix = "_R1_raw.fastq.gz"

    primer_trimmed_suffix = "_trimmed.fastq.gz"
    primer_trimmed_R1_suffix = "_R1_trimmed.fastq.gz"
    primer_trimmed_R2_suffix = "_R2_trimmed.fastq.gz"

elif args.assay == "Metagenomics":

    file_prefix = args.GLDS_ID + "_GMetagenomics_"

    # this one is only used for the raw data files
    raw_file_prefix = args.GLDS_ID + "_metagenomics_"

    assembly_based_dir = "Assembly-based_Processing/"
    assemblies_dir = "Assembly-based_Processing/assemblies/"
    genes_dir = "Assembly-based_Processing/predicted-genes/"
    annotations_and_tax_dir = "Assembly-based_Processing/annotations-and-taxonomy/"
    mapping_dir = "Assembly-based_Processing/read-mapping/"
    combined_output_dir = "Assembly-based_Processing/combined-outputs/"
    bins_dir = "Assembly-based_Processing/bins/"
    MAGs_dir = "Assembly-based_Processing/MAGs/"
    read_based_dir = "Read-based_Processing/"

    raw_suffix = "_HRremoved_raw.fastq.gz"
    raw_R1_suffix = "_R1_HRremoved_raw.fastq.gz"
    raw_R2_suffix = "_R2_HRremoved_raw.fastq.gz"

    older_raw_suffix = "_raw.fastq.gz"
    older_raw_R1_suffix = "_R1_raw.fastq.gz"
    older_raw_R2_suffix = "_R2_raw.fastq.gz"

    assembly_suffix = "-assembly.fasta"

if __name__ == "__main__":
    main()
