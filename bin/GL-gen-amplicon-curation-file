#!/usr/bin/env python

"""
This is a program for generating the table needed for GeneLab amplicon processed datasets for Curation.
"""

import os
import sys
import argparse
import textwrap
import pandas as pd
import tarfile
import zipfile

parser = argparse.ArgumentParser(description="This program generates table needed by curation for GeneLab amplicon \
                                             processed datasets.\
                                             Hard-coded variables that may need to be changed are at the top \
                                             of the script. It is expected to be run after `GL-validate-amplicon` and \
                                             `GL-gen-amplicon-readme` have been run successfully.")

required = parser.add_argument_group('required arguments')

required.add_argument("-g", "--GLDS-ID", help='GLDS ID (e.g. "GLDS-276")', action="store", required = True)
required.add_argument("-i", "--isa-zip", help='Appropriate ISA file for dataset (zipped)', action="store", required = True)
parser.add_argument("--primers-already-trimmed", help="Add this flag if primers were trimmed prior to GeneLab processing, \
                    therefore there are no trimmed sequence data.", action="store_true")
parser.add_argument("--single-ended", help="Add this flag if data are single-end sequencing.", action="store_true")
parser.add_argument("--type", help='Specify if ASVs or OTUs (default: "ASVs")', action="store", choices = ["ASVs", "OTUs"], default = "ASVs")
parser.add_argument("--map", help='Mapping file if samples come from more than one primer set (tab-delimited, first column holds sample IDs, second column holds the filename prefix of the outputs specific to that sample)', action="store")
parser.add_argument("--additional-prefix", help="Add any expected additional filename prefix that was added to the files that describe multiple samples (default: \"\")", default = "", action="store")


if len(sys.argv)==1:
    parser.print_help(sys.stderr)
    sys.exit(0)

args = parser.parse_args()

additional_prefix = str(args.additional_prefix)

### hard-coded stuff we might want to change ###
file_prefix = args.GLDS_ID + "_GAmplicon_"

raw_reads_dir = "Raw_Sequence_Data/"
fastqc_dir = "FastQC_Outputs/"
trimmed_reads_dir = "Trimmed_Sequence_Data/"
filtered_reads_dir = "Filtered_Sequence_Data/"
final_outputs_dir = "Final_Outputs/"

raw_suffix = "_raw.fastq.gz"
raw_R1_suffix = "_R1_raw.fastq.gz"
raw_R2_suffix = "_R2_raw.fastq.gz"
primer_trimmed_suffix = "_trimmed.fastq.gz"
primer_trimmed_R1_suffix = "_R1_trimmed.fastq.gz"
primer_trimmed_R2_suffix = "_R2_trimmed.fastq.gz"
filtered_suffix = "_filtered.fastq.gz"
filtered_R1_suffix = "_R1_filtered.fastq.gz"
filtered_R2_suffix = "_R2_filtered.fastq.gz"

processing_tar_file = additional_prefix + "processing_info.tar"


################################################################################

def main():

    ### temporary ###
    if args.primers_already_trimmed:
        report_failure("This program is not yet equipped to handle data where the primers were already trimmed :/ Get to it, Mike!")
    ##################

    check_for_file_and_contents(args.isa_zip)

    samples_in_ISA_order = get_samples_from_ISA(args.isa_zip)

    if args.map:
        map_tab = pd.read_csv(args.map, sep = "\t", names = ["sample", "prefix"])
        map_tab.set_index("sample", inplace = True)

    else:
        map_tab = None

    read_counts_df = get_read_counts_from_raw_multiqc(samples_in_ISA_order, map_tab)

#    gen_and_write_out_QC_metadata_tab(samples_in_ISA_order, read_counts_df)

    gen_and_write_out_filenames_table(samples_in_ISA_order, map_tab, read_counts_df)

################################################################################


# setting some colors
tty_colors = {
    'green' : '\033[0;32m%s\033[0m',
    'yellow' : '\033[0;33m%s\033[0m',
    'red' : '\033[0;31m%s\033[0m'
}


### functions ###
def color_text(text, color='green'):
    if sys.stdout.isatty():
        return tty_colors[color] % text
    else:
        return text


def wprint(text):
    """ print wrapper """

    print(textwrap.fill(text, width=80, initial_indent="  ", 
          subsequent_indent="  ", break_on_hyphens=False))


def report_failure(message, color = "yellow"):
    print("")
    wprint(color_text(message, color))
    print("\nJIRA-table generation failed.\n")
    sys.exit(1)


def check_for_file_and_contents(file_path):
    """ used by check_fastq_files function """

    if not os.path.exists(file_path):
        report_failure("The expected file '" + str(file_path) + "' does not exist.")
    if not os.path.getsize(file_path) > 0:
        report_failure("The file '" + str(file_path) + "' is empty.")


def get_samples_from_ISA(isa_file):
    """ gets the sample names in their order from the ISA zip file """

    zip_file = zipfile.ZipFile(isa_file)
    isa_files = zip_file.namelist()

    # getting wanted filename (those that start with "a_" seem to be what we want)
    wanted_file_list = [item for item in isa_files if item.startswith("a_")]
    if len(wanted_file_list) != 1:
        report_failure("We couldn't find the correct table in the ISA object.")

    wanted_file = wanted_file_list[0]

    df = pd.read_csv(zip_file.open(wanted_file), sep = "\t", usecols = ["Sample Name"])
    sample_IDs = df["Sample Name"].tolist()

    return(sample_IDs)


def get_read_counts_from_raw_multiqc(sample_names, mapping_tab):

    # these are in multiple files if there was a mapping input table
    if isinstance(mapping_tab, pd.DataFrame):
        unique_prefixes = mapping_tab.prefix.unique()

        # starting list to hold all dataframes we'll read in
        df_list = []

        # working through each one
        for prefix in unique_prefixes:

            curr_file_path = fastqc_dir + additional_prefix + prefix + "-raw_multiqc_data.zip"

            # making sure there are multiqc files for each unique prefix given in the mapping table
            check_for_file_and_contents(curr_file_path)

            # reading in
            zip_file = zipfile.ZipFile(curr_file_path)
            curr_df = pd.read_csv(zip_file.open("multiqc_general_stats.txt"), sep = "\t", usecols = [0,5])
            curr_df.columns = ["sample", "counts"]
            curr_df.set_index("sample", inplace = True)

            # adding to list
            df_list.append(curr_df)

        # combining tables
        df = pd.concat(df_list, axis = 0)

        return(df)

    else:
        zip_file = zipfile.ZipFile(fastqc_dir + additional_prefix + "raw_multiqc_data.zip")
        df = pd.read_csv(zip_file.open("multiqc_general_stats.txt"), sep = "\t", usecols = [0,5])
        df.columns = ["sample", "counts"]
        df.set_index("sample", inplace = True)
        return(df)


def gen_and_write_out_QC_metadata_tab(sample_names, read_counts_df):

    # generating needed order of names row writing out table (which has full file names) and as they are in the multiqc table, for finding them
    read_count_filename_list = []
    multiqc_name_list = []

    if not args.single_ended:

        for sample in sample_names:

            read_count_filename_list.append(sample + raw_R1_suffix)
            read_count_filename_list.append(sample + raw_R2_suffix)
            multiqc_name_list.append(sample + raw_R1_suffix.replace(".fastq.gz", ""))
            multiqc_name_list.append(sample + raw_R2_suffix.replace(".fastq.gz", ""))

    else:

        for sample in sample_names:

            read_count_filename_list.append(sample + raw_suffix)
            multiqc_name_list.append(sample + raw_suffix.replace(".fastq.gz", ""))


    # getting counts
    counts_list = []
    for entry in multiqc_name_list:

        counts_list.append(read_counts_df.at[entry, "counts"])

    # making counts output table
    out_df = pd.DataFrame()
    out_df["File Name"] = read_count_filename_list
    out_df["Number of Reads"] = counts_list

    out_df.to_csv(args.GLDS_ID + "-QC-metadata.tsv", sep = "\t", float_format='%.0f', index = False)


def get_prefix_from_map(sample_name, mapping_tab):

    # returning empty string if no mapping
    if not isinstance(mapping_tab, pd.DataFrame):
        return("")
    else:
        return(mapping_tab.at[sample_name, "prefix"] + "-")

def get_read_count_from_df(sample_name, read_counts_tab):

    if args.single_ended:

        return(read_counts_tab.at[str(sample_name) + "_raw", "counts"])

    else:

        return(read_counts_tab.at[str(sample_name) + "_R1_raw", "counts"])


def gen_and_write_out_filenames_table(sample_names, mapping_tab, read_count_tab):

    ## right now only for if primers were trimmed in workflow ##
    header_colnames = ["Sample Name", 
                       "Parameter Value[README]",
                       "Parameter Value[" + raw_reads_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[Read Depth]", 
                       "Unit",
                       "Parameter Value[" + trimmed_reads_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + filtered_reads_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + fastqc_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[" + final_outputs_dir.replace("_", " ").rstrip("/") + "]",
                       "Parameter Value[Processing Info]"]


    # entries and values that don't change per sample
    readme = file_prefix + additional_prefix + "README.txt"
    fastqc = [file_prefix + additional_prefix + "raw_multiqc_data.zip", 
              file_prefix + additional_prefix + "raw_multiqc_report.html", 
              file_prefix + additional_prefix + "filtered_multiqc_data.zip", 
              file_prefix + additional_prefix + "filtered_multiqc_report.html"]
    
    if args.type == "ASVs":
        rep_seq_output = file_prefix + additional_prefix + "ASVs.fasta"
    else:
        rep_seq_output = file_prefix + additional_prefix + "OTUs.fasta"

    final_outputs = [rep_seq_output, 
                     file_prefix + additional_prefix + "counts.tsv", 
                     file_prefix + additional_prefix + "read-count-tracking.tsv", 
                     file_prefix + additional_prefix + "taxonomy-and-counts.biom.zip", 
                     file_prefix + additional_prefix + "taxonomy-and-counts.tsv", 
                     file_prefix + additional_prefix + "taxonomy.tsv"]

    processing_info = file_prefix + additional_prefix + "processing_info.tar"

    read_count_unit = "read"
    read_count_term_source_ref = "SO"
    read_count_term_acc_number = "http://purl.obolibrary.org/obo/SO_0000150"

    building_df = pd.DataFrame(columns = header_colnames)

    for sample in sample_names:

        if args.single_ended:

            curr_raw_data = [file_prefix + sample + raw_suffix]
            curr_trimmed_data = [file_prefix + sample + primer_trimmed_suffix, 
                                 file_prefix + additional_prefix + "trimmed-read-counts.tsv", 
                                 file_prefix + additional_prefix + "cutadapt.log"]
            # curr_filt_data = [file_prefix + sample + filtered_suffix, 
            #                   file_prefix + additional_prefix + "filtered-read-counts.tsv", 
            #                   file_prefix + additional_prefix + "bbduk.log"]

            curr_filt_data = [file_prefix + sample + filtered_suffix, 
                              file_prefix + additional_prefix + "filtered-read-counts.tsv"]


        else:

            curr_raw_data = [file_prefix + sample + raw_R1_suffix, file_prefix + sample + raw_R2_suffix]
            curr_trimmed_data = [file_prefix + sample + primer_trimmed_R1_suffix, 
                                 file_prefix + sample + primer_trimmed_R2_suffix, 
                                 file_prefix + additional_prefix + "trimmed-read-counts.tsv", 
                                 file_prefix + additional_prefix + "cutadapt.log"]
            curr_filt_data = [file_prefix + sample + filtered_R1_suffix, 
                              file_prefix + sample + filtered_R2_suffix, 
                              file_prefix + additional_prefix + "filtered-read-counts.tsv"]

        curr_read_count = get_read_count_from_df(sample, read_count_tab)

        curr_row_as_list = [sample, 
                            readme,
                            ", ".join(curr_raw_data),
                            curr_read_count, 
                            read_count_unit,
                            ", ".join(curr_trimmed_data),
                            ", ".join(curr_filt_data),
                            ", ".join(fastqc),
                            ", ".join(final_outputs),
                            processing_info]

        # adding to building dataframe
        building_df.loc[len(building_df)] = curr_row_as_list

    if additional_prefix != "":
        building_df.to_csv(str(args.GLDS_ID) + "_" + additional_prefix + "associated-file-names.tsv", sep = "\t", index = False)
    else:
        building_df.to_csv(str(args.GLDS_ID) + "-associated-file-names.tsv", sep = "\t", index = False)


### commented out version below was when Curation wanted me to include "Term Source REF" and "Term Accession Number" columns
# def gen_and_write_out_filenames_table(sample_names, mapping_tab, read_count_tab):

#     ## right now only for if primers were trimmed in workflow ##
#     header_colnames = ["Sample Name", "Parameter Value[README]", "Term Source REF", "Term Accession Number",
#                       "Parameter Value[" + raw_reads_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                       "Parameter Value[Read Depth]", "Unit", "Term Source REF", "Term Accession Number",
#                       "Parameter Value[" + trimmed_reads_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                       "Parameter Value[" + filtered_reads_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                       "Parameter Value[" + fastqc_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                       "Parameter Value[" + final_outputs_dir.replace("_", " ").rstrip("/") + "]", "Term Source REF", "Term Accession Number",
#                       "Parameter Value[Processing Info]", "Term Source REF", "Term Accession Number"]

#     # entries and values that don't change per sample
#     readme = file_prefix + "README.txt"
#     fastqc = [file_prefix + "raw_multiqc_data.zip", file_prefix + "raw_multiqc_report.html", file_prefix + "filtered_multiqc_data.zip", file_prefix + "filtered_multiqc_report.html"]
    
#     if args.type == "ASVs":
#         rep_seq_output = file_prefix + "ASVs.fasta"
#     else:
#         rep_seq_output = file_prefix + "OTUs.fasta"

#     final_outputs = [rep_seq_output,  file_prefix + "counts.tsv", file_prefix + "read-count-tracking.tsv", file_prefix + "taxonomy-and-counts.biom.zip", file_prefix + "taxonomy-and-counts.tsv", file_prefix + "taxonomy.tsv"]

#     processing_info = file_prefix + "processing_info.tar"

#     read_count_unit = "read"
#     read_count_term_source_ref = "SO"
#     read_count_term_acc_number = "http://purl.obolibrary.org/obo/SO_0000150"

#     building_df = pd.DataFrame(columns = header_colnames)

#     for sample in sample_names:

#         if args.single_ended:

#             curr_raw_data = [file_prefix + sample + raw_suffix]
#             curr_trimmed_data = [file_prefix + sample + primer_trimmed_suffix, file_prefix + "trimmed-read-counts.tsv", file_prefix + "cutadapt.log"]
#             curr_filt_data = [file_prefix + sample + filtered_suffix, file_prefix + "filtered-read-counts.tsv", file_prefix + "bbduk.log"]

#         else:

#             curr_raw_data = [file_prefix + sample + raw_R1_suffix, file_prefix + sample + raw_R2_suffix]
#             curr_trimmed_data = [file_prefix + sample + primer_trimmed_R1_suffix, file_prefix + sample + primer_trimmed_R2_suffix, file_prefix + "trimmed-read-counts.tsv", file_prefix + "cutadapt.log"]
#             curr_filt_data = [file_prefix + sample + filtered_R1_suffix, file_prefix + sample + filtered_R2_suffix, file_prefix + "filtered-read-counts.tsv"]

#         curr_read_count = get_read_count_from_df(sample, read_count_tab)

#         curr_row_as_list = [sample, readme, "", "",
#                             ", ".join(curr_raw_data), "", "",
#                             curr_read_count, read_count_unit, read_count_term_source_ref, read_count_term_acc_number,
#                             ", ".join(curr_trimmed_data),"", "",
#                             ", ".join(curr_filt_data),"", "",
#                             ", ".join(fastqc), "", "",
#                             ", ".join(final_outputs), "", "",
#                             processing_info, "", ""]

#         # adding to building dataframe
#         building_df.loc[len(building_df)] = curr_row_as_list

#     if additional_prefix != "":
#         building_df.to_csv(str(args.GLDS_ID) + "_" + additional_prefix + "associated-file-names.tsv", sep = "\t", index = False)
#     else:
#         building_df.to_csv(str(args.GLDS_ID) + "-associated-file-names.tsv", sep = "\t", index = False)

    ## below here is a template from prior script for when i integrate dealing with those where primers weren't removed by genelab
    # with open(args.GLDS_ID + "-associated-file-names.tsv", "w") as output:

    #     # header
    #     if not args.primers_already_trimmed:

    #         output.write("Sample Name" + "\t" "README" + "\t" + raw_reads_dir.replace("_", " ").rstrip("/") +
    #             "\t" + trimmed_reads_dir.replace("_", " ").rstrip("/") +
    #             "\t" + filtered_reads_dir.replace("_", " ").rstrip("/") +
    #             "\t" + fastqc_dir.replace("_", " ").rstrip("/") +
    #             "\t" + final_outputs_dir.replace("_", " ").rstrip("/") +
    #             "\t" + "Processing Info" + "\n")

    #     else:

    #         output.write("Sample Name" + "\t" "README" + "\t" + raw_reads_dir.replace("_", " ").rstrip("/") +
    #             "\t" + filtered_reads_dir.replace("_", " ").rstrip("/") +
    #             "\t" + fastqc_dir.replace("_", " ").rstrip("/") +
    #             "\t" + final_outputs_dir.replace("_", " ").rstrip("/") +
    #             "\t" + "Processing Info" + "\n")


    #     # writing multiple lines for each sample entry (i don't know a better way to make this non-standard format)
    #     # if paired-end first
    #     if not args.single_ended:

    #         # this changes what's written out as there would be no primer-trimmed files or column for them
    #         if not args.primers_already_trimmed:

    #             for sample in sample_names:

    #                 output.write(str(sample) + "\t" + "README.txt" + "\t" +
    #                     sample + raw_R1_suffix + "\t" + sample + primer_trimmed_R1_suffix + "\t" +
    #                     sample + filtered_R1_suffix + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "raw_multiqc_data.zip" + "\t" + get_prefix_from_map(sample, mapping_tab) + args.type + ".fasta" + "\t" +
    #                     processing_tar_file +
    #                     "\n" +
    #                     "\t" + "\t" +
    #                     sample + raw_R2_suffix + "\t" + sample + primer_trimmed_R2_suffix + "\t" +
    #                     sample + filtered_R2_suffix + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "raw_multiqc_report.html.zip" + "\t" + get_prefix_from_map(sample, mapping_tab) + "counts.tsv" + "\t" + "" +
    #                     "\n" +
    #                     "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "trimmed-read-counts.tsv" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "filtered-read-counts.tsv" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "filtered_multiqc_data.zip" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "read-count-tracking.tsv" + "\t" + "" +
    #                     "\n" +
    #                     "\t" + "\t" + "\t" + get_prefix_from_map(sample, mapping_tab) + "cutadapt.log" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "filtered_multiqc_report.html.zip" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "taxonomy-and-counts.biom.zip" + "\t" + "" +
    #                     "\n" +
    #                     "\t" + "\t" + "\t" + "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "taxonomy.tsv" + "\t" + "" +
    #                     "\n"
    #                     "\t" + "\t" + "\t" + "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "taxonomy-and-counts.tsv" + "\t" + "" +
    #                     "\n" + "\n")

    #         else:

    #             for sample in sample_names:

    #                 output.write(str(sample) + "\t" + "README.txt" + "\t" +
    #                     sample + raw_R1_suffix + "\t" +
    #                     sample + filtered_R1_suffix + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "raw_multiqc_data.zip" + "\t" + get_prefix_from_map(sample, mapping_tab) + args.type + ".fasta" + "\t" +
    #                     processing_tar_file +
    #                     "\n" +
    #                     "\t" + "\t" +
    #                     sample + raw_R2_suffix + "\t" +
    #                     sample + filtered_R2_suffix + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "raw_multiqc_report.html.zip" + "\t" + get_prefix_from_map(sample, mapping_tab) +"counts.tsv" + "\t" + "" +
    #                     "\n" +
    #                     "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "filtered-read-counts.tsv" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "filtered_multiqc_data.zip" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "read-count-tracking.tsv" + "\t" + "" +
    #                     "\n" +
    #                     "\t" + "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "filtered_multiqc_report.html.zip" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "taxonomy-and-counts.biom.zip" + "\t" + "" +
    #                     "\n" +
    #                     "\t" + "\t" + "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "taxonomy.tsv" + "\t" + "" +
    #                     "\n"
    #                     "\t" + "\t" + "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "taxonomy-and-counts.tsv" + "\t" + "" +
    #                     "\n" + "\n")

    #     # now single-end
    #     else:

    #         # this changes what's written out as there would be no primer-trimmed files or column for them
    #         if not args.primers_already_trimmed:

    #             for sample in sample_names:

    #                 output.write(str(sample) + "\t" + "README.txt" + "\t" +
    #                     sample + raw_suffix + "\t" + sample + primer_trimmed_suffix + "\t" +
    #                     sample + filtered_suffix + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "raw_multiqc_data.zip" + "\t" + get_prefix_from_map(sample, mapping_tab) + args.type + ".fasta" + "\t" +
    #                     processing_tar_file +
    #                     "\n" +
    #                     "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "trimmed-read-counts.tsv" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "filtered-read-counts.tsv" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "raw_multiqc_report.html.zip" + "\t" + get_prefix_from_map(sample, mapping_tab) + "counts.tsv" + "\t" + "" +
    #                     "\n" +
    #                     "\t" + "\t" + "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "filtered_multiqc_data.zip" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "read-count-tracking.tsv" + "\t" + "" +
    #                     "\n" +
    #                     "\t" + "\t" + "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "filtered_multiqc_report.html.zip" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "taxonomy-and-counts.biom.zip" + "\t" + "" +
    #                     "\n" +
    #                     "\t" + "\t" + "\t" + "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "taxonomy.tsv" + "\t" + "" +
    #                     "\n"
    #                     "\t" + "\t" + "\t" + "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "taxonomy-and-counts.tsv" + "\t" + "" +
    #                     "\n" + "\n")

    #         else:

    #             for sample in sample_names:

    #                 output.write(str(sample) + "\t" + "README.txt" + "\t" +
    #                     sample + raw_suffix + "\t" +
    #                     sample + filtered_suffix + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "raw_multiqc_data.zip" + "\t" + get_prefix_from_map(sample, mapping_tab) + args.type + ".fasta" + "\t" +
    #                     processing_tar_file +
    #                     "\n" +
    #                     "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "filtered-read-counts.tsv" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "raw_multiqc_report.html.zip" + "\t" + get_prefix_from_map(sample, mapping_tab) + "counts.tsv" + "\t" + "" +
    #                     "\n" +
    #                     "\t" + "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "filtered_multiqc_data.zip" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "read-count-tracking.tsv" + "\t" + "" +
    #                     "\n" +
    #                     "\t" + "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "filtered_multiqc_report.html.zip" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "taxonomy-and-counts.biom.zip" + "\t" + "" +
    #                     "\n" +
    #                     "\t" + "\t" + "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "taxonomy.tsv" + "\t" + "" +
    #                     "\n"
    #                     "\t" + "\t" + "\t" + "\t" + "\t" +
    #                     get_prefix_from_map(sample, mapping_tab) + "taxonomy-and-counts.tsv" + "\t" + "" +
    #                     "\n" + "\n")


if __name__ == "__main__":
    main()

