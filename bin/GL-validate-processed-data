#!/usr/bin/env python

"""
This is a program for validating GeneLab processed datasets.
"""

import os
import sys
import argparse
import textwrap
import pandas as pd
import zipfile
import re
from statistics import mean, median


parser = argparse.ArgumentParser(description = "This program validates GeneLab processed datasets, currently designed to work with \
                                               Metagenomics, Amplicon, and MethylSeq datasets.")

required = parser.add_argument_group('required arguments')

required.add_argument("-a", "--assay", choices = ['Amplicon', 'Metagenomics', 'MethylSeq'], 
                      help = "Specifies which datatype (assay) is to be validated", action = "store", required = True)

required.add_argument("-g", "--GLDS-ID", help = 'GLDS ID (e.g. "GLDS-276")', action = "store", required = True)
required.add_argument("-s", "--sample-IDs-file", help = "Single-column file with unique file-name prefixes for each sample", action = "store", required = True)
parser.add_argument("-p", "--output-prefix", help = "Output additional file prefix if there is one", action = "store", default = "")
parser.add_argument("--single-ended", help = "Add this flag if data are single-end sequencing.", action = "store_true")
parser.add_argument("--primers-already-trimmed", help = "Add this flag if primers were trimmed prior to GeneLab processing, \
                    therefore there are no trimmed sequence data (only relevant for Amplicon)", action = "store_true")
parser.add_argument("--raw-reads-dir", help = "Specify location of raw reads directory (when not included, existence of raw reads will not be checked)", 
                    action = "store", default = "")
parser.add_argument("--R1-used-as-single-ended-data", help = "Provide this flag if processing only R1 reads as single-end (as the expected raw \
                    filename suffixes will have 'R1' in there; only relevant for Amplicon)", 
                    action = "store_true")


if len(sys.argv)==1:
    parser.print_help(sys.stderr)
    sys.exit(0)

args = parser.parse_args()


################################################################################

def main():

    # initializing log file
    setup_log()

    check_for_file_and_contents(args.output_prefix + "README.txt")
    append_message_to_log(f"    - populated {args.output_prefix + 'README.txt'} detected")

    check_expected_directories()
    
    sample_names = read_samples(args.sample_IDs_file)

    check_multiqc_outputs(sample_names)
    append_message_to_log(f"    - all expected samples found in raw and filtered multiqc files in {fastqc_dir}")

    check_fastq_files(sample_names)
    if args.assay == "Amplicon" and not args.primers_already_trimmed:
        append_message_to_log(f"    - all expected read files found in {trimmed_reads_dir}")
    append_message_to_log(f"    - all expected read files found in {filtered_reads_dir}")

    if args.assay == "Amplicon":

        check_amplicon_intermediate_log_files()

        check_amplicon_final_outputs()
        append_message_to_log(f"    - all expected files found in the {final_outputs_dir} directory")

        check_amplicon_processing_zip()

    elif args.assay == "Metagenomics":

        failed_assemblies_list = get_failed_assemblies()

        successful_assemblies_list = get_successful_assemblies(sample_names, failed_assemblies_list)

        check_assembly_based_outputs(sample_names, failed_assemblies_list, successful_assemblies_list)
        append_message_to_log(f"    - all expected files found in the {assembly_based_dir} directory")

        check_assembly_based_overview_table(sample_names, assembly_based_overview_table)

        check_read_based_outputs(expected_read_based_outputs)
        append_message_to_log(f"    - all expected files found in the {read_based_dir} directory")

        check_metagenomics_processing_zip(sample_names)

    elif args.assay == "MethylSeq":

        # checking for all expected files in bismark alignments directory
        check_dir_for_file_patterns(bismark_alignments_dir, bismark_alignment_files_expected_suffixes, sample_names = sample_names)
        append_message_to_log(f"    - all expected files found in the {bismark_alignments_dir} directory")

        # checking for bismark index zip
        check_for_file_and_contents(args.output_prefix + bismark_index_zip)
        append_message_to_log(f"    - populated {args.output_prefix + bismark_index_zip} detected")

        # checking for all expected files in bismark meth calls directory
        check_methylation_call_zips_for_file_patterns(bismark_meth_calls_dir, bismark_meth_call_files_expected_suffixes, sample_names = sample_names, expected_prefixes = bismark_meth_call_files_expected_prefixes)
        append_message_to_log(f"    - all expected files found in the {bismark_meth_calls_dir} directory and its contained zip archives")

        # checking for all expected files in bismark summary dir
        check_dir_for_files(bismark_summary_expected_files, bismark_summary_dir)
        check_dir_for_file_patterns(bismark_individual_summaries_dir, [".html"], sample_names = sample_names)
        append_message_to_log(f"    - all expected files found in the {bismark_summary_dir} directory")

        # checking for all expected base outputs in methylkit dir
        check_dir_for_files(methylkit_expected_files, methylkit_dir)
        append_message_to_log(f"    - all expected base files found in the {methylkit_dir} directory", one_return = True)
        
        # checking for expected outputs in all contrasts in methylkit dir
        global num_contrasts
        num_contrasts = check_methylkit_contrast_zips()
        append_message_to_log(f"        - and all expected files found for the {num_contrasts} detected contrast(s) in zip archives within this directory")
        
        # checking for all expected files in the reference genome dir
        check_dir_for_file_patterns(ref_files_dir, ref_files_expected_suffixes)
        append_message_to_log(f"    - all expected files found in the {ref_files_dir} directory")


    # checking for processing_info.zip
    check_for_file_and_contents(processing_zip_file)
    append_message_to_log(f"    - populated {processing_zip_file} detected")

    report_success()

    get_read_count_stats()

################################################################################

# setting some colors
tty_colors = {
    'green' : '\033[0;32m%s\033[0m',
    'yellow' : '\033[0;33m%s\033[0m',
    'red' : '\033[0;31m%s\033[0m'
}


### functions ###
def color_text(text, color='green'):
    if sys.stdout.isatty():
        return tty_colors[color] % text
    else:
        return text


def wprint(text):
    """ print wrapper """

    print(textwrap.fill(text, width = 80, initial_indent="  ", 
          subsequent_indent="  ", break_on_hyphens=False))


def report_failure(message, color = "yellow", log = True):
    print("")
    wprint(color_text(message, color))
    print("\nValidation failed.\n")

    if log:
        with open(validation_log, "a") as log:
            log.write(message + "\n" + "Validation failed." + "\n\n")

    sys.exit(1)


def setup_log():

    with open(validation_log, "w") as log:
        log.write(f"Performing baseline {args.assay} V+V as per: {V_V_guidelines_link}\n\n")


def append_message_to_log(message, one_return = False):

    with open(validation_log, "a") as log:

        if one_return:
            log.write(f"{message}\n")
        else:
            log.write(f"{message}\n\n")


def add_to_log_table(col1, col2):

    with open(validation_log, "a") as log:
        log.write(f"{col1}\t{col2}\n")


def report_success():
    print("")
    wprint(color_text("Validation has completed successfully :)", "green"))
    print(f"\n  Log written to: '{validation_log}'\n")

    with open(validation_log, "a") as log:

        log.write("   -----------------------------------------------------------------------------\n")
        log.write("                         Validation completed successfully." + "\n")
        log.write("   -----------------------------------------------------------------------------\n")

        # printing and writing to log file a message about the number of contrasts detected
        if args.assay == "MethylSeq":

            print("")
            wprint(color_text("**NOTE**", "yellow"))
            wprint(color_text(f"The number of contrast(s) detected was {num_contrasts}. Ensure that is correct by \
                               checking against the appropriate assay table.", "yellow"))
            print("")

            log.write("\n  **NOTE**\n")
            log.write(f"  The number of contrast(s) detected was {num_contrasts}. Ensure that is correct by \n")
            log.write(f"  checking against the appropriate assay table.\n")


def check_expected_directories():
    """ checks expected directories exist """

    for directory in expected_dirs:
        if not os.path.isdir(directory):

            report_failure("The directory '" + str(directory) + "' was expected but not found.")


def read_samples(file_path):
    """ reading unique sample names into list """

    with open(file_path) as f:
        sample_names = f.read().splitlines()

    return(sample_names)


def check_for_file_and_contents(file_path):
    """ used by various functions """

    if not os.path.exists(file_path):
        report_failure("The expected file '" + str(file_path) + "' does not exist.")
    if not os.path.getsize(file_path) > 0:
        report_failure("The file '" + str(file_path) + "' is empty.")


def check_fastq_files(sample_names):
    """ makes sure all expected read fastq files exist and hold something """

    for sample in sample_names:

        ## if paired-end
        if not args.single_ended:

            ## raw
            if args.raw_reads_dir != "":
                check_for_file_and_contents(os.path.join(args.raw_reads_dir, sample + raw_R1_suffix))
                check_for_file_and_contents(os.path.join(args.raw_reads_dir, sample + raw_R2_suffix))

            ## quality filtered
            check_for_file_and_contents(filtered_reads_dir + sample + filtered_R1_suffix)
            check_for_file_and_contents(filtered_reads_dir + sample + filtered_R2_suffix)

            ## trimmed if needed
            if args.assay == "Amplicon" and not args.primers_already_trimmed:
                check_for_file_and_contents(trimmed_reads_dir + sample + primer_trimmed_R1_suffix)
                check_for_file_and_contents(trimmed_reads_dir + sample + primer_trimmed_R2_suffix)


        ## if single-end
        else:

            ## raw
            if args.raw_reads_dir != "":
                check_for_file_and_contents(os.path.join(args.raw_reads_dir, sample + raw_suffix))

            ## filtered
            check_for_file_and_contents(filtered_reads_dir + sample + filtered_suffix)

            ## trimmed if needed
            if args.assay == "Amplicon" and not args.primers_already_trimmed:
                check_for_file_and_contents(trimmed_reads_dir + sample + primer_trimmed_suffix)


def check_multiqc_outputs(sample_names):
    """ makes sure all samples' read files are in the multiqc outputs """

    # checking raw
    zip_file = zipfile.ZipFile(fastqc_dir + str(output_prefix) + raw_multiqc_zip)

    # methylseq workflow has these nested, e.g., raw_multiqc_report/raw_multiqc_data/multiqc_general_stats.txt
    # right now, amplicon and metagenomics have raw_multiqc_report/multiqc_general_stats.txt
    if args.assay == "MethylSeq":
        df = pd.read_csv(zip_file.open(str(output_prefix) + raw_multiqc_zip.split(".")[0] + "/raw_multiqc_data/multiqc_general_stats.txt"), sep = "\t", usecols = ["Sample"])

    else:
        df = pd.read_csv(zip_file.open(str(output_prefix) + raw_multiqc_zip.split(".")[0] + "/multiqc_general_stats.txt"), sep = "\t", usecols = ["Sample"])


    file_prefixes_in_multiqc = df["Sample"].tolist()

    if not args.single_ended:

        R1_suffix = raw_R1_suffix.split(".")[0]
        R2_suffix = raw_R2_suffix.split(".")[0]

        for sample in sample_names:
            if not sample + R1_suffix in file_prefixes_in_multiqc:
                report_failure("The raw multiqc output is missing the expected '" + sample + R1_suffix + "' entry.")
            if not sample + R2_suffix in file_prefixes_in_multiqc:
                report_failure("The raw multiqc output is missing the expected '" + sample + R2_suffix + "' entry.")

    else:

        suffix = raw_suffix.split(".")[0]

        for sample in sample_names:
            if not sample + suffix in file_prefixes_in_multiqc and not sample in file_prefixes_in_multiqc:
                report_failure("The raw multiqc output is missing the expected '" + sample + suffix + "' entry.")

    # checking filtered
    zip_file = zipfile.ZipFile(fastqc_dir + str(output_prefix) + filtered_multiqc_zip)

    # methylseq workflow has these nested, e.g., filtered_multiqc_report/filtered_multiqc_data/multiqc_general_stats.txt
    # right now, amplicon and metagenomics have filtered_multiqc_report/multiqc_general_stats.txt
    if args.assay == "MethylSeq":
        df = pd.read_csv(zip_file.open(str(output_prefix) + filtered_multiqc_zip.split(".")[0] + "/filtered_multiqc_data/multiqc_general_stats.txt"), sep = "\t", usecols = ["Sample"])

    else:

        df = pd.read_csv(zip_file.open(str(output_prefix) + filtered_multiqc_zip.split(".")[0] + "/multiqc_general_stats.txt"), sep = "\t", usecols = ["Sample"])
    
    file_prefixes_in_multiqc = df["Sample"].tolist()

    if not args.single_ended:

        R1_suffix = filtered_R1_suffix.split(".")[0]
        R2_suffix = filtered_R2_suffix.split(".")[0]
        for sample in sample_names:
            if not sample + R1_suffix in file_prefixes_in_multiqc:
                report_failure("The filtered multiqc output is missing the expected '" + sample + R1_suffix + "' entry.")
            if not sample + R2_suffix in file_prefixes_in_multiqc:
                report_failure("The filtered multiqc output is missing the expected '" + sample + R2_suffix + "' entry.")

    else:

        suffix = filtered_suffix.split(".")[0]
        for sample in sample_names:
            if not sample + suffix in file_prefixes_in_multiqc and not sample in file_prefixes_in_multiqc:
                report_failure("The filtered multiqc output is missing the expected '" + sample + suffix + "' entry.")


def check_log_files():

    ## filtered
    output_files_present = get_files_in_dir(filtered_reads_dir)

    for entry in expected_filtered_outputs_or_suffixes:

        if not any(output_file.endswith(entry) for output_file in output_files_present):
            report_failure("An output file named or ending with '" + str(entry) + "' was expected but not found in " + str(filtered_reads_dir) + ".")


def check_general_fasta_format(file_path):

    if not os.path.getsize(file_path) > 0:
        report_failure("The fasta file '" + str(file_path) + "' is empty but isn't expected to be.")

    line_num = 0
    num_headers = 0
    num_seqs = 0

    with open(file_path) as in_file:

        for line in in_file:

            # keeping track of current line for reporting any problems
            line_num += 1

            if line.strip().startswith(">"):
                num_headers += 1
            else:
                num_seqs += 1

            if num_headers != num_seqs + 1 and num_headers != num_seqs:
                report_failure("Fasta file '" + str(file_path) + "' does not seem to be formatted properly. Problem detected at line " + str(line_num) + ".")


def get_files_in_dir(dir_path):

    return([f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))])


def get_directories_in_dir(dir_path):

    return([dir for dir in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, dir))])


def check_amplicon_intermediate_log_files():

    ## trimmed if needed
    if not args.primers_already_trimmed:

        output_files_present = get_files_in_dir(trimmed_reads_dir)

        for entry in expected_trimmed_outputs_or_suffixes:

            if not any(output_file.endswith(entry) for output_file in output_files_present):
                report_failure("An output file named or ending with '" + str(entry) + "' was expected but not found in " + str(trimmed_reads_dir) + ".")

    ## filtered
    output_files_present = get_files_in_dir(filtered_reads_dir)

    for entry in expected_filtered_outputs_or_suffixes:

        if not any(output_file.endswith(entry) for output_file in output_files_present):
            report_failure("An output file named or ending with '" + str(entry) + "' was expected but not found in " + str(filtered_reads_dir) + ".")


def check_amplicon_final_outputs():
    """ makes sure outputs exist and checks formatting """

    # getting list of files in output dir
    output_files_present = get_files_in_dir(final_outputs_dir)

    # making sure none of them are empty
    for output_file in output_files_present:
        check_for_file_and_contents(final_outputs_dir + output_file)

    # checking all desired output types exist
    for entry in expected_final_outputs_or_suffixes:

        if not any(output_file.endswith(entry) for output_file in output_files_present):
            report_failure("An output file named or ending with '" + str(entry) + "' was expected but not found in " + str(final_outputs_dir) + ".")

    # checking general fasta format is met
    fasta_files_in_output_dir = [output_file for output_file in output_files_present if output_file.endswith(".fasta")]

    for fasta_file in fasta_files_in_output_dir:
        check_general_fasta_format(final_outputs_dir + fasta_file)


def check_amplicon_processing_zip():
    """ this just makes sure a processing zip exists and at least has the Snakefile, as its contents can vary quite a bit """

    check_for_file_and_contents(processing_zip_file)

    with zipfile.ZipFile(processing_zip_file) as zip_obj:

        entries = zip_obj.namelist()

    target_substring = output_prefix.rstrip("-") + "/Snakefile"

    base_target = "/Snakefile"

    if not any(target_substring.lower() in string.lower() for string in entries):

        if not any(base_target.lower() in string.lower() for string in entries):
    
            report_failure("The '" + processing_zip_file + "' does not have a 'Snakefile' as expected.")


def get_failed_assemblies():

    failed_assemblies_list = []

    if os.path.exists(assemblies_dir + output_prefix + "Failed-assemblies.tsv"):
        with open(assemblies_dir + output_prefix + "Failed-assemblies.tsv") as failed_assemblies:
            for line in failed_assemblies:
                failed_assemblies_list.append(line.strip().split("\t")[0])

    return(failed_assemblies_list)


def get_successful_assemblies(sample_names, failed_assemblies_list):

    successful_assemblies = list(set(sample_names) - set(failed_assemblies_list))

    return(successful_assemblies)


def check_assembly_based_file(sample, file_path, failed_assemblies_list, assembly = True):

    if not os.path.exists(file_path):
        report_failure("The expected file '" + str(file_path) + "' does not exist.")

    if not os.path.getsize(file_path) > 0:

        # a sample can have no genes called even if the assembly produced contigs, so this is only throwing a warning if we are checking an assembly here
        if sample not in failed_assemblies_list and assembly == True:

            report_failure("The file '" + str(file_path) + "' is empty, but that sample isn't noted in the 'Failed-assemblies.tsv' file as it should be if the assembly failed.")


def check_assembly_based_genes_file(sample, file_path, failed_assemblies_list, assembly = True):
    """ 
    separate function for working with expected output genes files, to handle
    cases where assemblies can succeed while there are still no gene calls

    just checks the file isn't empty if it exists
    """

    if os.path.exists(file_path) and sample not in failed_assemblies_list:

        if not os.path.getsize(file_path) > 0:

            report_failure("The expected file '" + str(file_path) + "' exists, but appears to be empty when it shouldn't be.")


def check_assembly_based_outputs(sample_names, failed_assemblies_list, successful_assemblies_list):
    """ makes sure outputs exist and checks formatting """

    ## assemblies_dir ##
    for sample in sample_names:

        curr_fasta_path = assemblies_dir + sample + assembly_suffix

        # checking the file is present and not empty, unless it is noted in the Failed-assemblies file, then continuing to next sample
        if sample not in failed_assemblies_list:

            check_assembly_based_file(sample, curr_fasta_path, failed_assemblies_list)

            # checking the general fasta format if present
            check_general_fasta_format(curr_fasta_path)
    
    # making sure assembly summary file is there
    assembly_summary_path = assemblies_dir + output_prefix + "assembly-summaries.tsv"

    if not os.path.exists(assembly_summary_path):
        report_failure("The assembly summary file, " + str(assembly_summary_path) + ", is expected but was not found.")


    ## genes_dir ##
    predicted_gene_file_suffixes = ["-genes.faa", "-genes.gff", "-genes.fasta"]
    gene_fasta_suffixes = ["-genes.faa", "-genes.fasta"]

    # if any assemblies failed, these files won't exist for that assembly (they also may not exist if an assembly produced contigs too but not genes were called)
    for sample in sample_names:

        if sample not in failed_assemblies_list:

            for suffix in predicted_gene_file_suffixes:

                curr_file_path = genes_dir + sample + suffix

                # checking the file is not empty if it is present
                check_assembly_based_genes_file(sample, curr_file_path, failed_assemblies_list, assembly = False)

            # checking fasta format for those that exist
            for suffix in gene_fasta_suffixes:

                curr_fasta_path = genes_dir + sample + suffix

                if os.path.exists(curr_fasta_path) and os.path.getsize(curr_fasta_path) > 0:
                    check_general_fasta_format(curr_fasta_path)


    ## annotations_and_tax_dir ##
    annotations_suffixes = ["-gene-coverage-annotation-and-tax.tsv", "-contig-coverage-and-tax.tsv"]

    for sample in sample_names:

        for suffix in annotations_suffixes:

            curr_file_path = annotations_and_tax_dir + sample + suffix

            check_for_file_and_contents(curr_file_path)


    ## mapping_dir ##
    mapping_dir_suffixes_all_have = [".bam", "-metabat-assembly-depth.tsv"]
    mapping_info_suffix = "-mapping-info.txt"

    for sample in sample_names:

        for suffix in mapping_dir_suffixes_all_have:

            curr_file_path = mapping_dir + sample + suffix

            # checking the file is present and not empty unless it is noted in the Failed-assemblies file
            if sample not in failed_assemblies_list:
                check_assembly_based_file(sample, curr_file_path, failed_assemblies_list)

        # checking for mapping-info file for those that should have it
        if sample not in failed_assemblies_list:

            curr_file_path = mapping_dir + sample + mapping_info_suffix

            check_assembly_based_file(sample, curr_file_path, failed_assemblies_list)

    ## combined_output_dir ##
    for filename in expected_assembly_combined_outputs:

        curr_file_path = combined_output_dir + filename

        check_for_file_and_contents(curr_file_path)


    ## bins_dir ##
    # only if there were bins recovered
    output_files_present = get_files_in_dir(bins_dir)

    if output_files_present:

        output_fasta_bins = [filename for filename in output_files_present if filename.endswith(".fasta")]

        # checking for contents (checking fasta format not straightforward when there are softwraps, but don't want to remove them on these due to large contigs)
        for bin_file in output_fasta_bins:

            curr_file_path = bins_dir + bin_file

            if not os.path.getsize(curr_file_path) > 0:

                report_failure("The file '" + str(file_path) + "' is empty, but shouldn't be there if that's the case.")

        # making sure summary table is there if there are any bins
        if len(output_fasta_bins) > 0:

            bins_summary_path = bins_dir + output_prefix + "bins-overview.tsv"

            if not os.path.exists(bins_summary_path):

                report_failure("The bins summary file, " + str(bins_summary_path) + ", is expected but was not found.")


    ## MAGs_dir ##
    # only if there were MAGs recovered
    output_files_present = get_files_in_dir(MAGs_dir)

    if output_files_present:

        output_fasta_MAGs = [filename for filename in output_files_present if filename.endswith(".fasta")]

        # checking for contents (checking fasta format not straightforward when there are softwraps, but don't want to remove them on these due to large contigs)
        for MAG_file in output_fasta_MAGs:

            curr_file_path = MAGs_dir + MAG_file

            if not os.path.getsize(curr_file_path) > 0:

                report_failure("The file '" + str(file_path) + "' is empty, but shouldn't be there if that's the case.")

        # making sure summary table is there if there are any bins
        if len(output_fasta_bins) > 0:

            MAGs_summary_path = MAGs_dir + output_prefix + "MAGs-overview.tsv"

            if not os.path.exists(MAGs_summary_path):

                report_failure("The MAGs summary file, " + str(MAGs_summary_path) + ", is expected but was not found.")


def check_assembly_based_overview_table(expected_samples, overview_table_path):
    """ makes sure the output table exists and all input samples are in it """

    # first making sure it exists and is not empty
    check_for_file_and_contents(overview_table_path)

    # now making sure all samples are in there
    # reading in table and getting sample IDs in list
    overview_tab = pd.read_csv(overview_table_path, sep = "\t")
    samples_in_tab = overview_tab['Sample_ID'].tolist()

    missing_sample_IDs = []

    for sample in expected_samples:
        if sample not in samples_in_tab:
            missing_sample_IDs.append(sample)

    if len(missing_sample_IDs) > 0:
        report_failure("The assembly overview table, '" + overview_table_path + "', doesn't have all the samples expected to be there.")


def check_read_based_outputs(filenames):
    """ makes sure outputs exist and aren't empty """

    for file in filenames:

        check_for_file_and_contents(read_based_dir + file)


def check_metagenomics_processing_zip(samples):
    """ this makes sure a processing zip exists and has the expected core components """

    check_for_file_and_contents(processing_zip_file)

    with zipfile.ZipFile(processing_zip_file) as zip_obj:

        entries = zip_obj.namelist()

    for item in expected_zip_contents:

        if entries[0] + item not in entries:
            report_failure("The '" + str(processing_zip_file) + "' does not have '" + str(item) + "' as expected.")

    # checking log files
    for sample in samples:

        for suffix in expected_log_file_suffixes:

            target_log = logs_dir + sample + suffix

            if target_log not in entries:
                report_failure("The '" + str(processing_zip_file) + "' does not have the '" + str(target_log) + "' log file as expected.")


def check_methylation_call_zips_for_file_patterns(directory, expected_suffixes, sample_names, expected_prefixes):

    """ 
    checks that files are found in the methylation calls zips for each sample
    """

    # getting all files in target_dir
    all_files = get_files_in_dir(directory)

    # keeping only zips
    all_zips = [x for x in all_files if x.endswith(".zip")]

    # checking those with expected suffixes
    for sample_ID in sample_names:

        curr_zip_file = os.path.join(bismark_meth_calls_dir, sample_ID + bismark_meth_calls_zip_suffix)
        
        # making sure it exists and isn't empty
        check_for_file_and_contents(curr_zip_file)

        # getting files in zip
        with zipfile.ZipFile(curr_zip_file) as zip_obj:

            entries = zip_obj.namelist()

        # checking those with expected suffixes
        for suffix in expected_suffixes:
            
            hit = [ file for file in entries if file.endswith(suffix) ]

            if not hit:
                report_failure(f"An expected file ending with '*{suffix}' was not found in the {curr_zip_file} archive.")
        
        
        for prefix in expected_prefixes:

            curr_prefix = prefix + "_context_" + sample_ID + "_bismark"

            hit = [ file for file in entries if file.startswith(curr_prefix) ]

            if not hit:
                report_failure(f"An expected file beginning with '{prefix}*' was not found in the {curr_zip_file} archive.")


def check_dir_for_file_patterns(directory, expected_suffixes, sample_names = False, expected_prefixes = False):

    """ this is exclusive to MethylSeq """

    # getting all files in target dir
    all_files = get_files_in_dir(directory)

    # checking those with expected suffixes
    if not sample_names:

        for suffix in expected_suffixes:

            hit = [ file for file in all_files if file.endswith(suffix) ]

            if not hit:
                report_failure(f"An expected file ending with '*{suffix}' was not found in the {directory} directory.")

    else:

        for sample_ID in sample_names:

            curr_prefix = sample_ID + "_bismark"

            curr_files = [ file for file in all_files if file.startswith(curr_prefix) ]

            for suffix in expected_suffixes:

                hit = [ file for file in curr_files if file.endswith(suffix) ]

                if not hit:
                    report_failure(f"An expected file ending with '*{suffix}' for sample '{sample_ID}' was not found in the {directory} directory.")

    
        if expected_prefixes:

            # checking those with expected prefixes
            for sample_ID in sample_names:

                for prefix in expected_prefixes:

                    curr_prefix = prefix + "_context_" + sample_ID + "_bismark"

                    hit = [ file for file in all_files if file.startswith(curr_prefix) ]

                    if not hit:
                        report_failure(f"An expected file beginning with '{prefix}' for sample '{sample_ID}' was not found in the {directory} directory.")


def check_dir_for_files(expected_files, directory):

    # getting all files in target dir
    all_files = get_files_in_dir(directory)

    for file in expected_files:

        if file not in all_files:

            report_failure(f"The expected file '{file}' was not found in the {directory} directory.")


# def check_methylkit_subdirectories():

#     """ 
#     for every contrast subdirectory that exists, this checks the expected files are present
#     """

#     present_dirs = get_directories_in_dir(methylkit_dir)

#     num_contrasts = len(present_dirs)

#     for curr_directory in present_dirs:

#         check_dir_for_file_patterns(os.path.join(methylkit_dir, curr_directory), methylkit_subdir_expected_suffixes)

#     return(num_contrasts)

def check_methylkit_contrast_zips():

    """ 
    for every contrast zip that exists, this checks the expected files are present
    """

    present_zips = [file for file in get_files_in_dir(methylkit_dir) if file.endswith(".zip")]

    num_contrasts = len(present_zips)

    for zip_file in present_zips:

        curr_zip_file = os.path.join(methylkit_dir, zip_file)

        # getting files in zip
        with zipfile.ZipFile(curr_zip_file) as zip_obj:

            entries = zip_obj.namelist()

        # checking those with expected suffixes
        for suffix in methylkit_subdir_expected_suffixes:
            
            hit = [ file for file in entries if file.endswith(suffix) ]

            if not hit:
                report_failure(f"An expected file ending with '*{suffix}' was not found in the {curr_zip_file} archive.")

    return(num_contrasts)


def gen_stats(list_of_ints):

    """ returns min, max, mean, median of input integer list """

    min_val = min(list_of_ints)
    max_val = max(list_of_ints)

    mean_val = round(mean(list_of_ints), 2)
    median_val = int(median(list_of_ints))
    
    return(min_val, max_val, mean_val, median_val)


def get_read_count_stats():
    
    """ grabs read counts and summarizes """

    # for raw
    zip_file = zipfile.ZipFile(fastqc_dir + str(output_prefix) + raw_multiqc_zip)

    # methylseq workflow has these nested, e.g., raw_multiqc_report/raw_multiqc_data/multiqc_general_stats.txt
    # right now, amplicon and metagenomics have raw_multiqc_report/multiqc_general_stats.txt
    if args.assay == "MethylSeq":
        df = pd.read_csv(zip_file.open(str(output_prefix) + raw_multiqc_zip.split(".")[0] + "/raw_multiqc_data/multiqc_general_stats.txt"), sep = "\t", usecols = [5])

    else:
        df = pd.read_csv(zip_file.open(str(output_prefix) + raw_multiqc_zip.split(".")[0] + "/multiqc_general_stats.txt"), sep = "\t", usecols = [5])

    df.columns = ["counts"]
    raw_counts = df.counts.tolist()

    # getting rid of decimals
    raw_counts = [ int(round(i, 0)) for i in raw_counts ]

    raw_min, raw_max, raw_mean, raw_median = gen_stats(raw_counts)

    # for filtered
    zip_file = zipfile.ZipFile(fastqc_dir + str(output_prefix) + filtered_multiqc_zip)

    # methylseq workflow has these nested, e.g., filtered_multiqc_report/filtered_multiqc_data/multiqc_general_stats.txt
    # right now, amplicon and metagenomics have filtered_multiqc_report/multiqc_general_stats.txt
    if args.assay == "MethylSeq":
        df = pd.read_csv(zip_file.open(str(output_prefix) + filtered_multiqc_zip.split(".")[0] + "/filtered_multiqc_data/multiqc_general_stats.txt"), sep = "\t", usecols = [5])

    else:

        df = pd.read_csv(zip_file.open(str(output_prefix) + filtered_multiqc_zip.split(".")[0] + "/multiqc_general_stats.txt"), sep = "\t", usecols = [5])

    df.columns = ["counts"]
    filtered_counts = df.counts.tolist()

    # getting rid of decimals
    filtered_counts = [ int(round(i, 0)) for i in filtered_counts ]

    filtered_min, filtered_max, filtered_mean, filtered_median = gen_stats(filtered_counts)

    print("\n  Raw read count summary:")
    print("    {:<10} {:>0}".format("Min:", raw_min))
    print("    {:<10} {:>0}".format("Max:", raw_max))
    print("    {:<10} {:>0}".format("Mean:", raw_mean))
    print("    {:<10} {:>0}".format("Median:", raw_median))

    print("\n  Filtered read count summary:")
    print("    {:<10} {:>0}".format("Min:", filtered_min))
    print("    {:<10} {:>0}".format("Max:", filtered_max))
    print("    {:<10} {:>0}".format("Mean:", filtered_mean))
    print("    {:<10} {:>0}".format("Median:", filtered_median))
    print("")

    with open(validation_log, "a") as log:

        log.write("\n  Raw read count summary:")
        log.write("\n    {:<10} {:>0}".format("Min:", raw_min))
        log.write("\n    {:<10} {:>0}".format("Max:", raw_max))
        log.write("\n    {:<10} {:>0}".format("Mean:", raw_mean))
        log.write("\n    {:<10} {:>0}".format("Median:", raw_median))

        log.write("\n\n  Filtered read count summary:")
        log.write("\n    {:<10} {:>0}".format("Min:", filtered_min))
        log.write("\n    {:<10} {:>0}".format("Max:", filtered_max))
        log.write("\n    {:<10} {:>0}".format("Mean:", filtered_mean))
        log.write("\n    {:<10} {:>0}".format("Median:", filtered_median))
        log.write("\n")


### variable setup ###

# universal settings
output_prefix = str(args.output_prefix)
fastqc_dir = "FastQC_Outputs/"
filtered_reads_dir = "Filtered_Sequence_Data/"
raw_multiqc_zip = "raw_multiqc_report.zip"
filtered_multiqc_zip = "filtered_multiqc_report.zip"
processing_zip_file = output_prefix + "processing_info.zip"



# setting up when assay is amplicon
if args.assay == "Amplicon":

    V_V_guidelines_link = "https://genelab-tools.arc.nasa.gov/confluence/pages/viewpage.action?pageId=2428598"

    trimmed_reads_dir = "Trimmed_Sequence_Data/"
    final_outputs_dir = "Final_Outputs/"

    raw_suffix = "_raw.fastq.gz"
    raw_R1_suffix = "_R1_raw.fastq.gz"
    raw_R2_suffix = "_R2_raw.fastq.gz"

    if args.R1_used_as_single_ended_data:
        raw_suffix = raw_R1_suffix = "_R1_raw.fastq.gz"

    primer_trimmed_suffix = "_trimmed.fastq.gz"
    primer_trimmed_R1_suffix = "_R1_trimmed.fastq.gz"
    primer_trimmed_R2_suffix = "_R2_trimmed.fastq.gz"
    filtered_suffix = "_filtered.fastq.gz"
    filtered_R1_suffix = "_R1_filtered.fastq.gz"
    filtered_R2_suffix = "_R2_filtered.fastq.gz"

    expected_trimmed_outputs_or_suffixes = [output_prefix + "cutadapt.log", output_prefix + "trimmed-read-counts.tsv"]
    expected_filtered_outputs_or_suffixes = ["filtered-read-counts.tsv"]
    expected_final_outputs_or_suffixes = [".fasta", output_prefix + "counts.tsv", output_prefix + "taxonomy.tsv", ".biom.zip", output_prefix + "taxonomy-and-counts.tsv", output_prefix + "read-count-tracking.tsv"]

    expected_dirs = [fastqc_dir, filtered_reads_dir, final_outputs_dir]

    if args.raw_reads_dir != "":

        expected_dirs.append(args.raw_reads_dir)

    if not args.primers_already_trimmed:

        expected_dirs.append(trimmed_reads_dir)

    validation_log = str(args.GLDS_ID) + "_" + output_prefix + "amplicon-validation.log"

# setting up when assay is metagenomics
elif args.assay == "Metagenomics":

    V_V_guidelines_link = "https://genelab-tools.arc.nasa.gov/confluence/pages/viewpage.action?pageId=8225175"

    assembly_based_dir = "Assembly-based_Processing/"
    assemblies_dir = assembly_based_dir + "assemblies/"
    genes_dir = assembly_based_dir + "predicted-genes/"
    annotations_and_tax_dir = assembly_based_dir + "annotations-and-taxonomy/"
    mapping_dir = assembly_based_dir + "read-mapping/"
    combined_output_dir = assembly_based_dir + "combined-outputs/"
    bins_dir = assembly_based_dir + "bins/"
    MAGs_dir = assembly_based_dir + "MAGs/"
    read_based_dir = "Read-based_Processing/"
    logs_dir = output_prefix + "processing_info/logs/"

    expected_dirs = [fastqc_dir, filtered_reads_dir, assembly_based_dir,
                    assemblies_dir, genes_dir, annotations_and_tax_dir, mapping_dir,
                    combined_output_dir, bins_dir, MAGs_dir, read_based_dir]

    if args.raw_reads_dir != "":

        expected_dirs.append(args.raw_reads_dir)

    assembly_based_dirs = [assemblies_dir, genes_dir, annotations_and_tax_dir, mapping_dir,
                        combined_output_dir, bins_dir, MAGs_dir]

    raw_suffix = "_HRremoved_raw.fastq.gz"
    raw_R1_suffix = "_R1_HRremoved_raw.fastq.gz"
    raw_R2_suffix = "_R2_HRremoved_raw.fastq.gz"
    filtered_suffix = "_filtered.fastq.gz"
    filtered_R1_suffix = "_R1_filtered.fastq.gz"
    filtered_R2_suffix = "_R2_filtered.fastq.gz"

    assembly_suffix = "-assembly.fasta"

    expected_assembly_combined_outputs = [str(output_prefix) + "Combined-contig-level-taxonomy-coverages-CPM.tsv",
                                        str(output_prefix) + "Combined-gene-level-KO-function-coverages-CPM.tsv",
                                        str(output_prefix) + "Combined-gene-level-taxonomy-coverages-CPM.tsv",
                                        str(output_prefix) + "Combined-contig-level-taxonomy-coverages.tsv",
                                        str(output_prefix) + "Combined-gene-level-KO-function-coverages.tsv",
                                        str(output_prefix) + "Combined-gene-level-taxonomy-coverages.tsv"]

    assembly_based_overview_table = assembly_based_dir + str(output_prefix) + "Assembly-based-processing-overview.tsv"

    expected_read_based_outputs = [str(output_prefix) + "Gene-families-KO-cpm.tsv", 
                                str(output_prefix) + "Gene-families-cpm.tsv", 
                                str(output_prefix) + "Gene-families-grouped-by-taxa.tsv",
                                str(output_prefix) + "Gene-families.tsv", 
                                str(output_prefix) + "Metaphlan-taxonomy.tsv", 
                                str(output_prefix) + "Pathway-abundances-cpm.tsv",
                                str(output_prefix) + "Pathway-abundances-grouped-by-taxa.tsv", 
                                str(output_prefix) + "Pathway-abundances.tsv",
                                str(output_prefix) + "Pathway-coverages-grouped-by-taxa.tsv", 
                                str(output_prefix) + "Pathway-coverages.tsv"]

    expected_final_outputs_or_suffixes = [".fasta", "counts.tsv", "taxonomy.tsv", ".biom.zip", "taxonomy-and-counts.tsv", "read-count-tracking.tsv"]

    expected_zip_contents = ["Snakefile", "config.yaml", "envs/", "logs/", "scripts/", "unique-sample-IDs.txt"]

    expected_log_file_suffixes = ["-CAT.log", "-assembly.log", "-bam-summarize-and-metabat.log", "-bowtie2-build.log", 
                                "-bbduk.log", "-kofamscan.log", "-pileup.log", "-prodigal.log", "-humann3-run.log"]

    validation_log = str(args.GLDS_ID) + "_" + output_prefix + "metagenomics-validation.log"


# setting up when assay is methylseq
elif args.assay == "MethylSeq":

    V_V_guidelines_link = "https://genelab-tools.arc.nasa.gov/confluence/pages/viewpage.action?pageId=61865999"

    ref_files_dir = "Reference_Genome_Files/"
    bismark_alignments_dir = "Bismark_Alignments/"
    bismark_meth_calls_dir = "Methylation_Call_Data/"
    bismark_summary_dir = "Bismark_Summary/"
    methylkit_dir = "Differential_Methylation_Analysis_Data/"

    bismark_index_zip = "Bismark_Index.zip"

    expected_dirs = [fastqc_dir, filtered_reads_dir, ref_files_dir, bismark_alignments_dir,
                     bismark_meth_calls_dir, bismark_summary_dir]

    raw_suffix = "_raw.fastq.gz"
    raw_R1_suffix = "_R1_raw.fastq.gz"
    raw_R2_suffix = "_R2_raw.fastq.gz"
    filtered_suffix = "_trimmed.fastq.gz"
    filtered_R1_suffix = "_R1_trimmed.fastq.gz"
    filtered_R2_suffix = "_R2_trimmed.fastq.gz"

    bismark_alignment_files_expected_suffixes = [".nucleotide_stats.txt", "_qualimap.zip", "_sorted.bam"]

    bismark_meth_calls_zip_suffix = "_bismark_methylation_calls.zip"
    bismark_meth_call_files_expected_suffixes = [".bedGraph.gz", ".bismark.cov.gz", "CpG_report.txt.gz", ".M-bias.txt", "_splitting_report.txt", "cytosine_context_summary.txt"]
    bismark_meth_call_files_expected_prefixes = ["CHG", "CHH", "CpG"]

    bismark_summary_expected_files = ["bismark_summary_report.html", "bismark_summary_report.txt"]
    bismark_individual_summaries_dir = bismark_summary_dir + "/Individual_Sample_Reports/"

    ref_files_expected_suffixes = [".bed", "-map.tsv", ".gtf", ".fa"]

    methylkit_expected_files = ["base-level-percent-methylated.tsv", "tile-level-percent-methylated.tsv"]
    methylkit_subdir_expected_suffixes = ["hypermethylated-bases.tsv", "hypermethylated-tiles.tsv",
                                          "hypomethylated-bases.tsv", "hypomethylated-tiles.tsv",
                                          "diff-methylated-bases.tsv", "diff-methylated-tiles.tsv"]


    validation_log = str(args.GLDS_ID) + "_" + output_prefix + "methylseq-validation.log"



if __name__ == "__main__":
    main()
