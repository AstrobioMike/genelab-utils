#!/usr/bin/env python

"""
This is a program for generating a README.txt file for GeneLab processed datasets.
"""

import os
import sys
import argparse
import textwrap
import zipfile
import re


parser = argparse.ArgumentParser(description = "This program generates the corresponding README file for GeneLab processed datasets, \
                                             currently designed to work with metagenomics and amplicon datasets. It is intended to \
                                             only be run after `GL-validate-processed-data` has been run successfully.")

required = parser.add_argument_group('required arguments')

required.add_argument("-a", "--assay", choices = ['Amplicon', 'Metagenomics'], 
                      help = "Specifies which datatype (assay) this is for")

required.add_argument("-g", "--GLDS-ID", help = 'GLDS ID (e.g. "GLDS-69")', action = "store", required = True)
parser.add_argument("--output", help = 'Name of output file (default: "README.txt", with appended prefix if one is provided)', default = "README.txt")
parser.add_argument("-p", "--output-prefix", help = "Output additional file prefix if there is one", action = "store", default = "")
parser.add_argument("--name", help = 'Name of individual who performed the processing (default: "Michael D. Lee")', default = "Michael D. Lee")
parser.add_argument("--email", help = 'Email address of individual who performed the processing (default: "Mike.Lee@nasa.gov")', default = "Mike.Lee@nasa.gov")
parser.add_argument("--protocol-ID", help = 'Protocol document ID followed (default: assay dependent)', default = "")
parser.add_argument("--primers-already-trimmed", help = "Add this flag if primers were trimmed prior to GeneLab processing, \
                    therefore there are no trimmed sequence data (only relevant for Amplicon)", action = "store_true")
parser.add_argument("--raw-reads-dir", help = "Specify location of raw reads directory if they are to be included", action = "store", default = "")


if len(sys.argv)==1:
    parser.print_help(sys.stderr)
    sys.exit(0)

args = parser.parse_args()

latest_amplicon_DPPD = "GL-DPPD-7104-A"
latest_metagenomics_DPPD = "GL-DPPD-7107"

################################################################################

def main():

    if args.assay == "Amplicon":

        processing_zip_contents = get_processing_zip_contents()

        with open(output_file, "w") as output:

            write_amplicon_header(output, args.GLDS_ID, args.name, args.email, args.protocol_ID)

            write_amplicon_body(output, processing_zip_contents)


    elif args.assay == "Metagenomics":

        with open(output_file, "w") as output:

            write_metagenomics_header(output, args.GLDS_ID, args.name, args.email, args.protocol_ID)

            write_metagenomics_body(output)

################################################################################

### functions ###

def check_for_file_and_contents(file_path):
    """ used by get_processing_zip_contents function """

    if not os.path.exists(file_path):
        report_failure("The expected file '" + str(file_path) + "' does not exist.")
    if not os.path.getsize(file_path) > 0:
        report_failure("The file '" + str(file_path) + "' is empty.")


def get_processing_zip_contents():
    """ this gets the filenames that are in the processing_info.zip to add them to the readme """

    check_for_file_and_contents(processing_zip_file)

    with zipfile.ZipFile(processing_zip_file) as zip_obj:

        entries = zip_obj.namelist()
        entries.sort()

    return(entries)


def write_amplicon_header(output, GLDS_ID, name, email, protocol_ID):

    header = ["################################################################################\n",
              "{:<77} {:>0}".format("## This directory holds processed data for NASA " + str(GLDS_ID), "##\n"),
              "{:<77} {:>0}".format("## https://genelab-data.ndc.nasa.gov/genelab/accession/" + str(GLDS_ID) + "/", "##\n"),
              "{:<77} {:>0}".format("##", "##\n"),
              "{:<77} {:>0}".format("## Processed by " + str(name) + " (" + str(email) + ")", "##\n"),
              "{:<77} {:>0}".format("## Based on " + str(protocol_ID),  "##\n"),
              "################################################################################\n\n",
              "Summary of contents:\n\n"]

    output.writelines(header)


def write_amplicon_body(output, processing_zip_contents):

    # this file
    output.write("    {:<41} {:>0}".format("- " + str(output_file), "- this file\n\n"))

    # fastqc info
    output.write("    {:<41} {:>0}".format("- " + str(fastqc_dir), "- multiQC summary reports of FastQC runs\n\n"))

    # raw reads
    if args.raw_reads_dir != "":
        output.write("    {:<41} {:>0}".format("- " + str(args.raw_reads_dir), "- initial read fastq files\n\n"))

    # primer-trimmed reads if there are any
    if not args.primers_already_trimmed:
        output.write("    {:<41} {:>0}".format("- " + str(trimmed_reads_dir), "- primer-trimmed fastq files\n\n"))

    # quality-filtered reads
    output.write("    {:<41} {:>0}".format("- " + str(filtered_reads_dir), "- quality-filtered fastq files\n\n"))

    # outputs
    output.write("    {:<41} {:>0}".format("- " + str(final_outputs_dir), "- primary output files (may or may not have additional prefix)\n"))
    output.write("        {:<37} {:>0}".format("- *.fasta", "- fasta file of recovered sequences\n"))
    output.write("        {:<37} {:>0}".format("- *counts.tsv", "- count table of sequences across samples\n"))
    output.write("        {:<37} {:>0}".format("- *taxonomy.tsv", "- assigned taxonomy of recovered sequences\n"))
    output.write("        {:<37} {:>0}".format("- *taxonomy-and-count.tsv", "- combined table of counts and taxonomy\n"))
    output.write("        {:<37} {:>0}".format("- *taxonomy-and-count.biom.zip", "- biom-formatted output of counts and taxonomy\n"))
    output.write("        {:<37} {:>0}".format("- *read-count-tracking.tsv", "- read counts at each processing step\n\n"))

    # processing info
    output.write("    {:<41} {:>0}".format("- " + str(processing_zip_file), "- zip archive holding info related to processing\n"))
    for item in processing_zip_contents:

        num_levels = item.count("/")

        if num_levels > 1 and not item.endswith("/"):
            out_item = re.sub(r'^.*/', '', str(item))
        elif num_levels == 1 and not item.endswith("/"):
            out_item = re.sub(r'^.*/', '', str(item))
        elif num_levels > 1:
            out_item = re.sub(r'^[^/]*/', '', str(item))
        else:
            out_item = str(item)

        if item.endswith('/'):
            num_levels -= 1

        num_spaces = num_levels * 4

        output.write("        " + " " * num_spaces + "- " + out_item + "\n")

    output.write("\n")


def write_metagenomics_header(output, GLDS_ID, name, email, protocol_ID):

    header = ["################################################################################\n",
              "{:<77} {:>0}".format("## This directory holds processed data for NASA " + str(GLDS_ID), "##\n"),
              "{:<77} {:>0}".format("## https://genelab-data.ndc.nasa.gov/genelab/accession/" + str(GLDS_ID) + "/", "##\n"),
              "{:<77} {:>0}".format("##", "##\n"),
              "{:<77} {:>0}".format("## Processed by " + str(name) + " (" + str(email) + ")", "##\n"),
              "{:<77} {:>0}".format("## Based on " + str(protocol_ID),  "##\n"),
              "################################################################################\n\n",
              "Summary of contents:\n\n"]

    output.writelines(header)


def write_metagenomics_body(output):

    # this file
    output.write("    {:<63} {:>0}".format("- " + str(output_file), "- this file\n\n"))

    # fastqc info
    output.write("    {:<63} {:>0}".format("- " + str(fastqc_dir), "- multiQC summary reports of FastQC runs\n\n"))

    # raw reads
    if args.raw_reads_dir != "":
        output.write("    {:<63} {:>0}".format("- " + str(args.raw_reads_dir), "- initial read fastq files\n\n"))

    # quality-filtered reads
    output.write("    {:<63} {:>0}".format("- " + str(filtered_reads_dir), "- quality-filtered fastq files\n\n"))

    # outputs
    output.write("    {:<63} {:>0}".format("- " + str(assembly_based_dir), "- results generated from assembly-based approach\n\n"))

    output.write("        {:<59} {:>0}".format("- " + output_prefix + str("Assembly-based-processing-overview.tsv"), "- Assembly-based overview per sample\n\n"))
    
    output.write("        {:<59} {:>0}".format("- " + str(assemblies_dir), "- per-sample assembly files and info\n"))
    output.write("            {:<55} {:>0}".format("- *-assembly.fasta", "- fasta files of individual sample assemblies\n"))
    output.write("            {:<55} {:>0}".format("- " + output_prefix + "assembly-summaries.tsv", "- table of all assemblies' summary statistics\n"))
    output.write("            {:<55} {:>0}".format("- " + output_prefix + "Failed-assemblies.tsv", "- samples that didn't assemble any contigs (if any)\n\n"))

    output.write("        {:<59} {:>0}".format("- " + str(genes_dir), "- per-sample predicted gene files\n"))
    output.write("            {:<55} {:>0}".format("- *.faa", "- gene amino-acid sequences\n"))
    output.write("            {:<55} {:>0}".format("- *.fasta", "- gene nucleotide sequences\n"))
    output.write("            {:<55} {:>0}".format("- *.gff", "- predicted genes in general feature format\n\n"))

    output.write("        {:<59} {:>0}".format("- " + str(annotations_and_tax_dir), "- per-sample Kegg Orthology (KO) annotations, taxonomy, and coverages\n"))
    output.write("            {:<55} {:>0}".format("- *-gene-coverage-annotation-tax.tsv", "- tables with gene coverage, annotation, and taxonomy info\n"))
    output.write("            {:<55} {:>0}".format("- *-contig-coverage-and-tax.tsv", "- tables with contig coverage and taxonomy info\n\n"))

    output.write("        {:<59} {:>0}".format("- " + str(mapping_dir), "- per-sample bam, coverage, and mapping info files\n"))
    output.write("            {:<55} {:>0}".format("- *.bam", "- bam files\n"))
    output.write("            {:<55} {:>0}".format("- *.tsv", "- coverage files used for metabat2 binning\n"))
    output.write("            {:<55} {:>0}".format("- *.txt", "- stdout from bowtie2 mapping\n\n"))

    if os.path.exists(assembly_based_dir + bins_dir):
        output.write("        {:<59} {:>0}".format("- " + str(bins_dir), "- genomic bins recovered (if any)\n"))
        output.write("            {:<55} {:>0}".format("- *.fasta", "- fasta files of bins recovered\n"))
        output.write("            {:<55} {:>0}".format("- " + output_prefix + "bins-overview.tsv", "- summary stats of bins recovered\n\n"))

    if os.path.exists(assembly_based_dir + MAGs_dir):
        output.write("        {:<59} {:>0}".format("- " + str(MAGs_dir), "- high-quality Metagenome-Assembled Genomes recovered (if any; > 90% est. comp., < 10% est. redundancy)\n"))
        output.write("            {:<55} {:>0}".format("- *.fasta", "- fasta files of MAGs\n"))
        output.write("            {:<55} {:>0}".format("- " + output_prefix + "MAGs-overview.tsv", "- summary stats of MAGs including GTDB taxonomy\n"))
        output.write("            {:<55} {:>0}".format("- " + output_prefix + "MAG-level-KO-annotations.tsv", "- KO functional annotations associated with each MAG\n"))
        output.write("            {:<55} {:>0}".format("- " + output_prefix + "MAG-KEGG-Decoder*", "- KEGG-Decoder summaries of MAG functional annotations\n\n"))


    output.write("        {:<59} {:>0}".format("- " + str(combined_output_dir), "- summary outputs with all samples combined\n"))
    output.write("            {:<55} {:>0}".format("- " + output_prefix + "Combined-gene-level-KO-function-coverages.tsv", "- table of combined KO function coverages\n"))
    output.write("            {:<55} {:>0}".format("- " + output_prefix + "Combined-gene-level-KO-function-coverages-CPM.tsv", "- table of combined KO function coverages, normalized to coverage per million\n"))
    output.write("            {:<55} {:>0}".format("- " + output_prefix + "Combined-gene-level-taxonomy-coverages.tsv", "- table of combined, gene-level taxonomy coverages\n"))
    output.write("            {:<55} {:>0}".format("- " + output_prefix + "Combined-gene-level-taxonomy-coverages-CPM.tsv", "- table of combined, gene-level taxonomy coverages, normalized to coverage per million\n"))
    output.write("            {:<55} {:>0}".format("- " + output_prefix + "Combined-contig-level-taxonomy-coverages.tsv", "- table of combined, contig-level taxonomy coverages\n"))
    output.write("            {:<55} {:>0}".format("- " + output_prefix + "Combined-contig-level-taxonomy-coverages-CPM.tsv", "- table of combined, contig-level taxonomy coverages, normalized to coverage per million\n\n"))

    output.write("    {:<63} {:>0}".format("- " + str(read_based_dir), "- results generated from read-based approach\n"))
    output.write("        {:<59} {:>0}".format("- " + output_prefix + "Gene-families.tsv", "- gene-family abundances\n"))
    output.write("        {:<59} {:>0}".format("- " + output_prefix + "Gene-families-grouped-by-taxa.tsv", "- gene-family abundances grouped by taxa\n"))
    output.write("        {:<59} {:>0}".format("- " + output_prefix + "Gene-families-cpm.tsv", "- gene-family abundances normalized to copies-per-million\n"))
    output.write("        {:<59} {:>0}".format("- " + output_prefix + "Gene-families-KO-cpm.tsv", "- KO term abundances normalized to copies-per-million\n"))
    output.write("        {:<59} {:>0}".format("- " + output_prefix + "Pathway-abundances.tsv", "- pathway abundances\n"))
    output.write("        {:<59} {:>0}".format("- " + output_prefix + "Pathway-abundances-grouped-by-taxa.tsv", "- pathway abundances grouped by taxa\n"))
    output.write("        {:<59} {:>0}".format("- " + output_prefix + "Pathway-abundances-cpm.tsv", "- pathway abundances normalized to copies-per-million\n"))
    output.write("        {:<59} {:>0}".format("- " + output_prefix + "Pathway-coverages.tsv", "- pathway coverages\n"))
    output.write("        {:<59} {:>0}".format("- " + output_prefix + "Pathway-coverages-grouped-by-taxa.tsv", "- pathway coverages grouped by taxa\n"))
    output.write("        {:<59} {:>0}".format("- " + output_prefix + "Metaphlan-taxonomy.tsv", "- metaphlan estimated taxonomic relative abundances\n\n"))

    # processing info
    output.write("    {:<63} {:>0}".format("- " + str(processing_zip_file), "- zip archive holding info related to processing\n"))
    output.write("        {:<59} {:>0}".format("- unique-sample-IDs.txt", "- single-column file of unique sample identifiers\n"))
    output.write("        {:<59} {:>0}".format("- Snakefile", "- Snakemake workflow file\n"))
    output.write("        {:<59} {:>0}".format("- config.yaml", "- configuration file for workflow\n"))
    output.write("        {:<59} {:>0}".format("- envs/", "- conda environments for workflow\n"))
    output.write("        {:<59} {:>0}".format("- scripts/", "- scripts used by workflow\n"))
    output.write("        {:<59} {:>0}".format("- logs/", "- log files of specific commands run\n"))

    output.write("\n")



### variable setup ###

# universal settings
output_prefix = str(args.output_prefix)
fastqc_dir = "FastQC_Outputs/"
processing_zip_file = output_prefix + "processing_info.zip"

output_file = output_prefix + str(args.output)

if args.assay == "Amplicon":

    trimmed_reads_dir = "Trimmed_Sequence_Data/"
    filtered_reads_dir = "Filtered_Sequence_Data/"
    final_outputs_dir = "Final_Outputs/"

    if args.protocol_ID == "":

        args.protocol_ID = latest_amplicon_DPPD

elif args.assay == "Metagenomics":

    filtered_reads_dir = "Filtered_Sequence_Data/"

    assembly_based_dir = "Assembly-based_Processing/"
    assemblies_dir = "assemblies/"
    genes_dir = "predicted-genes/"
    annotations_and_tax_dir = "annotations-and-taxonomy/"
    mapping_dir = "read-mapping/"
    combined_output_dir = "combined-outputs/"
    bins_dir = "bins/"
    MAGs_dir = "MAGs/"
    read_based_dir = "Read-based_Processing/"

    if args.protocol_ID == "":

        args.protocol_ID = latest_metagenomics_DPPD


if __name__ == "__main__":
    main()
