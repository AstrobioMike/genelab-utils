#!/usr/bin/env python

"""
This is a program for validating GeneLab raw data
"""

import os
import sys
import argparse
import textwrap
import pandas as pd
import glob
import re
import subprocess

parser = argparse.ArgumentParser(description = "This program validates GeneLab raw datasets, checks or creates md5s, and \
                                                runs fastqc combines its outputs with multiqc. For version info, run `GL-version`.",
                                 epilog = "Ex. usage: GL-validate-raw-data -g GLDS-480 -s samples.txt")

required = parser.add_argument_group('required arguments')

required.add_argument("-g", "--GLDS-ID", help = 'GLDS ID (e.g. "GLDS-480")', action = "store", required = True)
required.add_argument("-n", "--number-of-samples", help = "The expected number of samples here", action = "store", type = int, required = True)

parser.add_argument("--md5-file", help = "Include a file holding all md5sum results if avaiable (one will be created if not)", \
                    action = "store", default = "")
parser.add_argument("--single-ended", help = "Add this flag if data are single-end sequencing", action = "store_true")
parser.add_argument("--keep-fastqc-files", help = "Add this flag if wanting to keep individual-sample fastqc files", action = "store_true")


if len(sys.argv)==1:
    parser.print_help(sys.stderr)
    sys.exit(0)

args = parser.parse_args()


# currently hard-coded things we may want to adjust
R1_designations = ["_R1_"]
R2_designations = ["_R2_"]
extensions = [".fq", ".fastq"]
standard_GL_R1_suffix = "_R1_raw.fastq.gz"
standard_GL_R2_suffix = "_R2_raw.fastq.gz"
standard_GL_SE_suffix = "_raw.fastq.gz"

md5_output_file = str(args.GLDS_ID) + "-raw-read-md5s.tsv"
fastqc_threads = 8 # allocates 250 MB memory per thread, so 8 is 2 GB
log_out = str(args.GLDS_ID) + "-raw-validation-log.txt"

output_table_filename = str(args.GLDS_ID) + "-raw-validation-summary.tsv"


################################################################################

def main():

    # making sure files for all expected samples can be found, 
    # and making a map of unique name to file name
    if args.md5_file:
        # if an md5 file was provided, the map can be parsed from there,
        # for now moving forward as if not provided
        print("  \n\nNOT READY FOR MD5 INPUT YET!!\n\n")
        exit(1)

    else:

        # creating mapping dictionary
        map_dict = check_files_and_gen_unique_ID_to_file_map(args.number_of_samples)

        # # starting validation output table
        out_tab = start_validation_output_table(map_dict)

        # # renaming files to be in our convention (e.g., _R1_raw.fastq.gz for forward, or "_raw.fastq.gz" 
        # # if single-end), also adding to output table
        map_dict, out_tab = rename_files(map_dict, out_tab)

        # # generating md5s, writing to a collective md5 file, and adding to summary table
        map_dict, out_tab = gen_md5s(map_dict, out_tab)

        # # run fastqc and multiqc
        run_fastqc_and_multiqc(map_dict)

        # add read counts to summary output table
        out_tab = parse_and_add_read_counts(map_dict, out_tab)

        # writing output summary table
        out_tab.reset_index(inplace = True)
        out_tab = out_tab.rename(columns = {'index': 'unique_ID'})
        out_tab.to_csv(output_table_filename, sep = "\t", index = False)


################################################################################

# setting some colors
tty_colors = {
    'green' : '\033[0;32m%s\033[0m',
    'yellow' : '\033[0;33m%s\033[0m',
    'red' : '\033[0;31m%s\033[0m'
}


### functions ###

def color_text(text, color='green'):
    if sys.stdout.isatty():
        return tty_colors[color] % text
    else:
        return text


def wprint(text):
    """ print wrapper """

    print(textwrap.fill(text, width=80, initial_indent="  ",
          subsequent_indent="  ", break_on_hyphens=False))


def report_failure(message, color = "red"):
    print("")
    wprint(color_text(message, color))
    print("\nRaw data V+V failed.\n")
    sys.exit(1)


def print_notification(message, color = "yellow"):
    print("")
    wprint(color_text(message, color))
    print("")


def check_files_and_gen_unique_ID_to_file_map(expected_count):
    """ 
    checks there are the appropriate number of files based on expected number of samples, and
    returns a dictionary with unique IDs as keys and list of read files as values 
    """

    # getting all files in current directory that end with .gz
    gz_files = glob.glob("*.gz")
    # keeping only those that have .fq or .fastq in them
    gz_files = [file for file in gz_files if any(extension in file for extension in extensions)]

    num_files = len(gz_files)

    # checking the expected number of files were detected
    if not args.single_ended:

        if num_files != args.number_of_samples * 2:
            report_failure("We expected " + str(args.number_of_samples * 2) + " read files, but " + str(num_files) + " were detected.")
    else:

        if num_files != args.number_of_samples:
            report_failure("We expected " + str(args.number_of_samples) + " read files, but " + str(num_files) + " were detected.")



    # making sure they have the expected R1/R2 designations if paired-end data
    if not args.single_ended:

        forward_read_files_list = [file for file in gz_files if any(designation in file for designation in R1_designations)]
        reverse_read_files_list = [file for file in gz_files if any(designation in file for designation in R2_designations)]

        num_forward_read_files = len(forward_read_files_list)
        num_reverse_read_files = len(reverse_read_files_list)

        if args.number_of_samples != num_forward_read_files:

            report_failure("We expected " + str(args.number_of_samples) + " forward read files, but " + str(num_forward_read_files) + " were found based on currently allowed paired-read designations (e.g. these formats: " + ', '.join(R1_designations) + "). Share this use-case with Mike to expand functionality as others are needed.")

        if args.number_of_samples != num_reverse_read_files:

            report_failure("We expected " + str(args.number_of_samples) + " reverse read files, but " + str(num_forward_read_files) + " were found based on currently allowed paired-read designations (e.g. these formats: " + ', '.join(R2_designations) + "). Share this use-case with Mike to expand functionality as others are needed.")

    # making a dictionary with unique IDs as keys and a list of read files as values
    if not args.single_ended:

        # finding which read-pair designation we are using here (superfluous until we add more, but will be needed then)
        # putting a check that only one is found (one mechanism to hopefully prevent if these patterns are just part of a 
        # sample name, and not a read-pair designation)

        num_forward_designations_found = 0
        num_reverse_designations_found = 0

        for designation in R1_designations:

            if designation in forward_read_files_list[0]:
                R1_designation = designation
                num_forward_designations_found += 1

        num_reverse_designations_found = 0
        for designation in R2_designations:

            if designation in reverse_read_files_list[0]:
                R2_designation = designation
                num_reverse_designations_found += 1

        if num_forward_designations_found != 1 or num_reverse_designations_found != 1:
            report_failure("We're having trouble delineating read-pairs based on the currently allowed formats (e.g. these formats: " + ', '.join(R1_designations) + "). Share this use-case with Mike to expand functionality.")


        # making dictionary with unique IDs as keys and list of read-files as values
        map_dict = {}
        for file in forward_read_files_list:

            # getting unique portion of names prior to _R?_ designation
            target_string_pattern = R1_designation + ".*$"
            unique_ID = re.sub(target_string_pattern, "", file)

            map_dict[unique_ID] = [file]

        for file in reverse_read_files_list:

            # getting unique portion of names prior to _R?_ designation
            target_string_pattern = R2_designation + ".*$"
            unique_ID = re.sub(target_string_pattern, "", file)

            map_dict[unique_ID].append(file)

        return(map_dict)

    # now the case if single-end data
    else:

        # find out which fq/fastq extension is being used here so we can get unique names prior to that (or to be able to rename in our convention if needed)
        # making sure not more than one is detected
        num_extensions_found = 0
        for possible_extension in extensions:

            if possible_extension in gz_files[0]:
                used_extension = possible_extension
                num_extensions_found += 1

        if num_extensions_found != 1:
            report_failure("We're not seeing the expected fastq extensions (e.g. any of these: " + ', '.join(extensions) + "). Share this use-case with Mike to expand functionality.")

        # making dictionary with unique IDs as keys and read file as value
        map_dict = {}
        for file in gz_files:

            # getting unique portion of names prior to fasta extension
            target_string_pattern = used_extension + ".*$"
            unique_ID = re.sub(target_string_pattern, "", file)

            map_dict[unique_ID] = [file]

        return(map_dict)


def start_validation_output_table(map_dict):
    """ starts output table and adds unique IDs and file names """

    if not args.single_ended:
        out_tab = pd.DataFrame.from_dict(map_dict, orient = "index", columns = ['orig_R1_filename', 'orig_R2_filename'])

    else:
        out_tab = pd.DataFrame.from_dict(map_dict, orient = "index", columns = ['orig_read_filename'])

    return(out_tab)


def rename_files(map_dict, out_tab):
    """ 
    this renames the files to have the conventional GeneLab suffixes 
    e.g., _R1_raw.fastq.gz
    """

    renamed_map_dict = {}

    if not args.single_ended:

        for key in map_dict.keys():

            R1_orig_name = map_dict[key][0]
            R2_orig_name = map_dict[key][1]

            R1_new_name = str(key) + str(standard_GL_R1_suffix)
            R2_new_name = str(key) + str(standard_GL_R2_suffix)

            os.rename(R1_orig_name, R1_new_name)
            os.rename(R2_orig_name, R2_new_name)

            renamed_map_dict[key] = [R1_new_name]
            renamed_map_dict[key].append(R2_new_name)

        # adding new names to output table
        new_tab = pd.DataFrame.from_dict(renamed_map_dict, orient = "index", columns = ["new_R1_filename", "new_R2_filename"])
        out_tab = pd.concat([out_tab, new_tab], axis = 1)

    else:

        for key in map_dict.keys():

            orig_name = map_dict[key][0]

            new_name = str(key) + str(standard_GL_SE_suffix)

            os.rename(orig_name, new_name)

            renamed_map_dict[key] = [new_name]

        # adding new names to output table
        new_tab = pd.DataFrame.from_dict(renamed_map_dict, orient = "index", columns = ["new_filename"])
        out_tab = pd.concat([out_tab, new_tab], axis = 1)

    return(renamed_map_dict, out_tab)


def gen_md5s(map_dict, out_tab):
    """ generates md5 checksum file for all read files """

    print_notification("Generating md5s...")
    # making dictionary holding md5 values
    md5_dict = {}

    if not args.single_ended:

        for key in map_dict.keys():

            forward_read_file = map_dict[key][0]
            reverse_read_file = map_dict[key][1]

            forward_md5_out = subprocess.run(['md5sum', forward_read_file], stdout = subprocess.PIPE).stdout.decode('utf-8')
            reverse_md5_out = subprocess.run(['md5sum', reverse_read_file], stdout = subprocess.PIPE).stdout.decode('utf-8')

            # getting rid of filename (these come like this '3s9ckfjie  file.txt' from command-line md5sum program)
            forward_md5_out = re.sub(" .*$", "", forward_md5_out).strip()
            reverse_md5_out = re.sub(" .*$", "", reverse_md5_out).strip()

            md5_dict[key] = [forward_md5_out]
            md5_dict[key].append(reverse_md5_out)

        # adding md5 values to output table
        new_tab = pd.DataFrame.from_dict(md5_dict, orient = "index", columns = ["R1_md5", "R2_md5"])
        out_tab = pd.concat([out_tab, new_tab], axis = 1)

        # writing out md5 file
        with open(md5_output_file, "w") as md5_out_file:
            
            for key in map_dict.keys():
                
                forward_file_name = map_dict[key][0]
                reverse_file_name = map_dict[key][1]

                forward_file_md5 = md5_dict[key][0]
                reverse_file_md5 = md5_dict[key][1]

                md5_out_file.write(str(forward_file_md5) + "\t" + str(forward_file_name) + "\n")
                md5_out_file.write(str(reverse_file_md5) + "\t" + str(reverse_file_name) + "\n")

    else:

        for key in map_dict.keys():

            read_file = map_dict[key][0]

            md5_out = subprocess.run(['md5sum', read_file], stdout = subprocess.PIPE).stdout.decode('utf-8')

            # getting rid of filename (these come like this '3s9ckfjie  file.txt' from command-line md5sum program)
            md5_out = re.sub(" .*$", "", md5_out).strip()
            md5_dict[key] = [md5_out]

        # adding md5 values to output table
        new_tab = pd.DataFrame.from_dict(md5_dict, orient = "index", columns = ["read_file_md5"])
        out_tab = pd.concat([out_tab, new_tab], axis = 1)

        # writing out md5 file
        with open(md5_output_file, "w") as md5_out_file:
            
            for key in map_dict.keys():
                
                file_name = map_dict[key][0]

                file_md5 = md5_dict[key][0]

                md5_out_file.write(str(file_md5) + "\t" + str(file_name) + "\n")

    return(map_dict, out_tab)


def run_fastqc_and_multiqc(map_dict):
    """ runs fastqc on all read files """

    print_notification("Running fastqc...")

    # making list of all files
    list_of_all_files = []

    for key in map_dict.keys():

        list_of_all_files += map_dict[key]

    # converting list to space-delimited string for passing to subprocess
    list_for_subprocess = " ".join(list_of_all_files)
    # building command since i can't figure out how to pass a list or space-delimited string of positional
    # arguments to subprocess.run
    with open(log_out, "w") as log:
        log.write("\n\n    FASTQC log:\n\n")

    fastqc_command = "fastqc -t " + str(fastqc_threads) + " " + str(list_for_subprocess) + " >> " + str(log_out) + " 2>&1"

    subprocess.run([fastqc_command], shell = True)

    # running multiqc
    with open(log_out, "a") as log:
        log.write("\n\n    MULTIQC log:\n\n")

    multiqc_command = "multiqc ./ >> " + str(log_out) + " 2>&1"

    subprocess.run([multiqc_command], shell = True)

    # removing fastqc files unless specified not to
    if not args.keep_fastqc_files:
        
        for file in glob.glob("*fastqc*"):
            os.remove(file)


def parse_and_add_read_counts(map_dict, out_tab):
    """ this gets the read counts from the multiqc output and adds them to the summary table """

    df = pd.read_csv("multiqc_data/multiqc_general_stats.txt", sep = "\t", usecols = [0,5])
    df.columns = ["sample", "counts"]
    df.set_index("sample", inplace = True)

    # making dictionary of counts 
    counts_dict = {}

    if not args.single_ended:

        for key in map_dict.keys():

            forward_read_multiqc_ID = re.sub(".fastq.gz", "", map_dict[key][0])
            reverse_read_multiqc_ID = re.sub(".fastq.gz", "", map_dict[key][1])

            num_forward_reads = round(df.loc[forward_read_multiqc_ID, 'counts'])
            num_reverse_reads = round(df.loc[reverse_read_multiqc_ID, 'counts'])

            counts_dict[key] = [num_forward_reads]
            counts_dict[key].append(num_reverse_reads)

            # adding value for if counts are equal
            if num_forward_reads == num_reverse_reads:

                counts_dict[key].append("True")

            else:

                counts_dict[key].append("False")

        # adding read counts to output table
        new_tab = pd.DataFrame.from_dict(counts_dict, orient = "index", columns = ["R1_num_reads", "R2_num_reads", "R1_and_R2_num_reads_equal"])
        out_tab = pd.concat([out_tab, new_tab], axis = 1)

    # handling if single-end
    else:

        for key in map_dict.keys():

            read_multiqc_ID = re.sub(".fastq.gz", "", map_dict[key][0])
        
            num_reads = round(df.loc[read_multiqc_ID, 'counts'])

            counts_dict[key] = [num_reads]

        # adding read counts to output table
        new_tab = pd.DataFrame.from_dict(counts_dict, orient = "index", columns = ["read_counts"])
        out_tab = pd.concat([out_tab, new_tab], axis = 1)

    return(out_tab)


if __name__ == "__main__":
    main()

